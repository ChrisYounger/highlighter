{
   "abstract": {
      "alias": "excerpt",
      "category": "formatting",
      "commentcheat": "Show a summary of up to 5 lines for each search result.",
      "description": "Produce an abstract -- a summary or brief representation -- of the text of search results.  The original text is replaced by the summary, which is produced by a scoring mechanism.  If the event is larger than the selected maxlines, those with more terms and more terms on adjacent lines are preferred over those with fewer terms.  If a line has a search term, its neighboring lines also partially match, and may be returned to provide context. When there are gaps between the selected lines, lines are prefixed with \"...\". \\p\\\n           If the text of a result has fewer lines or an equal number of lines to maxlines, no change will occur.\\i\\\n           * <maxlines> accepts values from 1 - 500. \\i\\\n           * <maxterms> accepts values from 1 - 1000.",
      "examplecheat": "... |abstract maxlines=5",
      "related": "highlight",
      "shortdesc": "Shortens the text of results to a brief summary representation.",
      "syntax": "abstract (maxterms=<int>)? (maxlines=<int>)?",
      "tags": "condense summarize summary outline pare prune shorten skim snip sum trim",
      "usage": "public"
   },
   "accum": {
      "category": "fields::add",
      "comment1": "Save the running total of \"count\" in a field called \"total_count\".",
      "description": "For each event where <field> is a number, keep a running total of the sum of this number and write it out to either the same field, or a new field if specified.",
      "example1": "... | accum count AS total_count",
      "related": "autoregress, delta, streamstats, trendline",
      "shortdesc": "Keeps a running total of a specified numeric field.",
      "syntax": "accum <field> (AS <field>)?",
      "tags": "total sum accumulate",
      "usage": "public"
   },
   "addcoltotals": {
      "category": "reporting",
      "comment1": "Compute the sums of all the fields, and put the sums in a summary event called \"change_name\".",
      "comment2": "Add a column total for two specific fields in a table.",
      "comment3": "Augment a chart with a total of the values present.",
      "description": "Appends a new result to the end of the search result set.\nThe result contains the sum of each numeric field or you can specify which fields\nto summarize. Results are displayed on the Statistics tab. If the labelfield argument\nis specified, a column is added to the statistical results table with the name\nspecified.",
      "example1": "... | addcoltotals labelfield=change_name label=ALL",
      "example2": "sourcetype=access_* | table userId bytes avgTime duration | addcoltotals bytes duration",
      "example3": "index=_internal source=*metrics.log group=pipeline |stats avg(cpu_seconds) by processor |addcoltotals labelfield=processor",
      "related": "stats",
      "shortdesc": "Appends a new result to the end of the search result set.",
      "syntax": "addcoltotals (labelfield=<field>)? (label=<string>)? <field-list>?",
      "tags": "total add calculate sum",
      "usage": "public"
   },
   "addinfo": {
      "category": "fields::add",
      "comment": "Add information about the search to each event.",
      "description": "Adds global information about the search to each event.  The addinfo command is primarily an internal component of summary indexing. \\i\\\n             Currently the following fields are added: \\i\\\n             \"info_min_time\"    - the earliest time bound for the search \\i\\\n             \"info_max_time\"    - the latest time bound for the search \\i\\\n             \"info_search_id\"   - query id of the search that generated the event \\i\\\n             \"info_search_time\" - time when the search was executed.",
      "example": "... |addinfo",
      "related": "search",
      "shortdesc": "Add fields that contain common information about the current search.",
      "syntax": "addinfo",
      "tags": "search info",
      "usage": "public"
   },
   "addtotals": {
      "category": "reporting",
      "comment1": "Compute the sums of the numeric fields of each results.",
      "comment2": "Compute the sums of the numeric fields that match the given list, and save the sums in the field \"sum\".",
      "comment3": "Compute the sums of all the fields, and put the sums in a summary event called \"change_name\".",
      "commentcheat": "Calculate the sums of the numeric fields of each result, and put the sums in the field \"sum\".",
      "description": "If \"row=t\" (default if invoked as 'addtotals') for each result, computes the arithmetic sum of all\n             numeric fields that match <field-list> (wildcarded field list).\n             If list is empty all fields are considered.\n             The sum is placed in the specified field or \"Total\" if none was specified.\n             If \"col=t\" (default if invoked as 'addcoltotals'), adds a new result at the end that represents the sum of each field.\n             LABELFIELD, if specified, is a field that will be added to this summary\n             event with the value set by the 'label' option.",
      "example1": "... | addtotals",
      "example2": "... | addtotals fieldname=sum foobar* *baz*",
      "example3": "... | addtotals col=t labelfield=change_name label=ALL",
      "examplecheat": "... | addtotals fieldname=sum",
      "related": "stats",
      "shortdesc": "Computes the sum of all numeric fields for each result.",
      "syntax": "addtotals (row=<bool>)? (col=<bool>)? (labelfield=<field>)? (label=<string>)? (fieldname=<field>)? <field-list>",
      "tags": "total add calculate sum",
      "usage": "public"
   },
   "analyzefields": {
      "alias": "af",
      "category": "reporting",
      "comment1": "Analyze the numerical fields to predict the value of \"is_activated\".",
      "description": "Using <field> as a discrete random variable, analyze all *numerical* fields to determine the ability for each of those fields to \"predict\" the value of the classfield.\n             In other words, analyzefields determines the stability of the relationship between values in the target classfield and numeric values in other fields. \\i\\\n             As a reporting command, analyzefields consumes all input results, and generates one output result per identified numeric field. \\i\\\n             For best results, classfield should have 2 distinct values, although multi-class analysis is possible.",
      "example1": "... | analyzefields classfield=is_activated",
      "related": "anomalousvalue",
      "shortdesc": "Finds degree of correlation between a target discrete field and other numerical fields.",
      "syntax": "analyzefields classfield=<field>",
      "tags": "analyze predict",
      "usage": "public beta"
   },
   "anomalies": {
      "category": "results::filter",
      "comment1": "Return only anomalous events.",
      "comment2": "Show most interesting events first, ignoring any in the blacklist 'boringevents'.",
      "comment3": "Use with transactions to find regions of time that look unusual.",
      "description": "Determines the degree of \"unexpectedness\" of an event's field\n   value, based on the previous MAXVALUE events.  By default it\n   removes events that are well-expected (unexpectedness >\n   THRESHOLD). The default THRESHOLD is 0.01.  If LABELONLY is true,\n   no events are removed, and the \"unexpectedness\" attribute is set\n   on all events.  The FIELD analyzed by default is \"_raw\".\n   By default, NORMALIZE is true, which normalizes numerics.  For cases\n   where FIELD contains numeric data that should not be normalized, but\n   treated as categories, set NORMALIZE=false. The\n   BLACKLIST is a name of a csv file of events in\n   $SPLUNK_HOME/var/run/splunk/<BLACKLIST>.csv, such that any incoming\n   events that are similar to the blacklisted events are treated as\n   not anomalous (i.e., uninteresting) and given an unexpectedness\n   score of 0.0.  Events that match blacklisted events with a\n   similarity score above BLACKLISTTHRESHOLD (defaulting to 0.05) are\n   marked as unexpected.  The inclusion of a 'by' clause, allows the\n   specification of a list of fields to segregate results for anomaly\n   detection.  For each combination of values for the specified\n   field(s), events with those values are treated entirely separately.\n   Therefore, 'anomalies by source' will look for anomalies in each\n   source separately -- a pattern in one source will not affect that\n   it is anomalous in another source.",
      "example1": "... | anomalies",
      "example2": "... | anomalies blacklist=boringevents | sort -unexpectedness",
      "example3": "... | transaction maxpause=2s | anomalies",
      "related": "anomalousvalue, cluster, kmeans, outlier",
      "shortdesc": "Computes an \"unexpectedness\" score for an event.",
      "syntax": "anomalies (threshold=<num>)? (labelonly=<bool>)? (normalize=<bool>)? (maxvalues=<int>)? (field=<field>)? (blacklist=<filename>)? (blacklistthreshold=<num>)? (<by-clause>)?",
      "tags": "anomaly unusual odd irregular dangerous unexpected outlier",
      "usage": "public"
   },
   "anomalousvalue": {
      "category": "reporting",
      "comment1": "Return only uncommon values.",
      "commentcheat": "Return events with uncommon values.",
      "description": "Identifies or summarizes the values in the data that are anomalous either by frequency of occurrence\n             or number of standard deviations from the mean.  If a field-list is given, only those fields are\n             considered.  Otherwise all non internal fields are considered. \\p\\\n             For fields that are considered anomalous, a new field is added with the following scheme.\n             If the field is numeric, e.g. \\\"size\\\",  the new field will be  \\\"Anomaly_Score_Num(size)\\\".\n             If the field is non-numeric, e.g. \\\"name\\\", the new field will be \\\"Anomaly_Score_Cat(name)\\\".",
      "example1": "... | anomalousvalue",
      "examplecheat": "... | anomalousvalue action=filter pthresh=0.02",
      "related": "af, anomalies, cluster, kmeans, outlier",
      "shortdesc": "Finds and summarizes irregular, or uncommon, search results.",
      "syntax": "anomalousvalue <av-option>* <anovalue-action-option>? <anovalue-pthresh-option>? <field-list>?",
      "tags": "anomaly unusual odd irregular dangerous unexpected",
      "usage": "public"
   },
   "anomalydetection": {
      "category": "streaming, reporting",
      "comment": "The way probabilities are computed is called the Naive Bayes method, which means the individual fields are considered independent.\n         This is a simplification to make the command reasonably fast.",
      "comment1": "Return only anomalous events.",
      "comment2": "Return a short summary of how many anomalous events are there and some other statistics such as the threshold value used to detect them.",
      "description": "Identify anomalous events by computing a probability for each event and then detecting unusually small probabilities.\n             The probability is defined as the product of the frequencies of each individual field value in the event.\n             For categorical fields, the frequency of a value X is the number of times X occurs divided by the total number of events.\n             For numerical fields, we first build a histogram for all the values, then compute the frequency of a value X\n             as the size of the bin that contains X divided by the number of events.\n             Missing values are treated by adding a special value and updating its count just like a normal value.\n             Histograms are built using the standard Scott's rule to determine the bin width.",
      "example1": "... | anomalydetection",
      "example2": "... | anomalydetection action=summary",
      "related": "anomalies, anomalousvalue, outlier, cluster, kmeans",
      "shortdesc": "Find anomalous events in a set of search results.",
      "syntax": "anomalydetection <anoma-method-option>? <anoma-action-option>? <anoma-pthresh-option>? <anoma-cutoff-option>? <field-list>?",
      "tags": "anomaly unusual odd irregular dangerous unexpected Bayes",
      "usage": "public"
   },
   "append": {
      "category": "results::append",
      "comment": "Append the current results with the tabular results of errors.",
      "description": "Append the results of a subsearch as additional results at the end of the current results.",
      "example": "... | chart count by category1 | append [search error | chart count by category2]",
      "related": "appendcols, join, set",
      "shortdesc": "Appends the results of a subsearch results to the current results.",
      "syntax": "append (<subsearch-options>)? <subsearch>",
      "tags": "append join combine unite combine",
      "usage": "public"
   },
   "appendcols": {
      "category": "fields::add",
      "comment": "Search for \"404\" events and append the fields in each event to the previous search results.",
      "description": "Appends fields of the results of the subsearch into input search results by combining the external fields of the subsearch (fields that do not start with '_') into the current results.  The first subsearch result is merged with the first main result, the second with the second, and so on.  If option override is false (default), if a field is present in both a subsearch result and the main result, the main result is used.  If it is true, the subsearch result's value for that field is used.",
      "example": "... | appendcols [search 404]",
      "related": "append, join, set",
      "shortdesc": "Appends the fields of the subsearch results to current results, first results to first result, second to second, etc.",
      "syntax": "appendcols (override=<bool> | <subsearch-options>)? <subsearch>",
      "tags": "append join combine unite",
      "usage": "public"
   },
   "appendpipe": {
      "category": "results::append",
      "comment": "Append subtotals for each action across all users",
      "description": "Appends the result of the subpipeline applied to the current result set to results.",
      "example": "index=_audit | stats count by action user | appendpipe [stats sum(count) as count by action | eval user = \"ALL USERS\"] | sort action",
      "related": "append appendcols join set",
      "syntax": "appendpipe (run_in_preview=<bool>)? [<subpipeline>]",
      "tags": "append join combine unite combine",
      "usage": "public"
   },
   "archivebuckets": {
      "comment1": "To execute archivebuckets manually",
      "description": "Archives Splunk buckets to Hadoop. The archivebuckets command is usually run on a schedule by Splunk. The optional argument \"forcerun=1\" is needed when the search is run manually.\nThe optional argument \"retries\" can be used to specify the number of attempts that should be made to assure all buckets have archived. The default is 1.\nFailures (e.g. due to lost network connectivity) will be retried the next time the command is run.\narchivebuckets will generate virtual index information and spawn a new distributed search named copybuckets, which will archive buckets on each search peer connected to this Splunk instance.\nFor more information, visit docs.splunk.com.",
      "example1": "| archivebuckets forcerun=1",
      "shortdesc": "Archives Splunk buckets to Hadoop.",
      "syntax": "archivebuckets [forcerun=1] [retries=N]",
      "tags": "a, ar, arc, archive, buckets, bucket, roll, roller",
      "usage": "public"
   },
   "arules": {
      "category": "streaming, reporting",
      "comment1": "Running arules with default support (=3) and confidence (=.5)\n        The minimum number of fields is 2. There is no maximum restriction.",
      "comment2": "The sup option must be a positive integer\n        The conf option must be a float between 0 and 1\n        In general, the higher the support, the less noisy the output will be. However, setting the support too high may exclude too much\n        useful data in some circumstances. The conf option should be at least 0.5, otherwise the associations will not be significant. The higher\n        the conf, the more significant the associations will be, but at the expense of retaining less associations.",
      "description": "Finding association rules between values. This is the algorithm behind most online\n           shopping websites. When a customer buys an item, these sites are able to recommend\n           related items that other customers also buy when they buy the first one. Arules finds such relationships and not only for\n           shopping items but any kinds of fields. Note that stricly speaking, arules does not find relationships between fields, but rather\n           between the values of the fields.",
      "example1": "... | arules field1 field2 field3",
      "example2": "... | arules sup=3 conf=.6 field1 field2 field3",
      "related": "associate, correlate",
      "shortdesc": "Finds the association rules between field values.",
      "syntax": "arules (<arules-option> )* <fields>",
      "tags": "associate contingency correlate correspond dependence independence",
      "usage": "public"
   },
   "associate": {
      "category": "reporting",
      "comment1": "Analyze all fields to find a relationship.",
      "comment2": "Analyze all events from host \"reports\" and return results associated with each other.",
      "commentcheat": "Return results associated with each other (that have at least 3 references to each other).",
      "description": "Searches for relationships between pairs of fields.  More specifically, this command tries to identify\n             cases where the entropy of field1 decreases significantly based on the condition of field2=value2.\n             field1 is known as the target key and field2 the reference key and value2 the reference value.\n             If a list of fields is provided, analysis will be restrict to only those fields.  By default all fields\n             are used.",
      "example1": "... | associate",
      "example2": "host=\"reports\" | associate supcnt=50 supfreq=0.2 improv=0.5",
      "examplecheat": "... | associate supcnt=3",
      "related": "correlate, contingency",
      "shortdesc": "Identifies correlations between fields.",
      "syntax": "associate (<associate-option> )* <field-list>?",
      "tags": "associate contingency correlate connect link correspond dependence independence",
      "usage": "public"
   },
   "audit": {
      "category": "administrative",
      "commentcheat": "View information in the \"audit\" index.",
      "description": "View audit trail information stored in the local \"audit\" index. Also validate signed audit events while checking for gaps and tampering.",
      "examplecheat": "index=audit | audit",
      "related": "metadata",
      "shortdesc": "Returns audit trail information that is stored in the local audit index.",
      "syntax": "audit",
      "tags": "audit trail security",
      "usage": "public"
   },
   "autoregress": {
      "alias": "ar",
      "category": "reporting",
      "comment1": "Calculate a moving average of event size; the first N average numbers are omitted by eval since summing null fields results in null.",
      "comment2": "For each event, copy the 2nd, 3rd, 4th, and 5th previous values of the 'count' field into the respective fields 'count_p2', 'count_p3', 'count_p4', and 'count_p5'.",
      "comment3": "For each event, copy the 3rd previous value of the 'foo' field into the field 'oldfoo'.",
      "description": "Sets up data for auto-regression (e.g. moving average) by copying one or more of the previous values for <field> into each event.  If <newfield> is provided, one prior value will be copied into <newfield> from a count of 'p' events prior.  In this case, 'p' must be a single integer.  If <newfield> is not provided, the single or multiple values will be copied into fields named '<field>_p<p-val>'.  In this case 'p' may be a single integer, or a range <p_start>-<p_end>.  For a range, the values will be copied from 'p_start' events prior to 'p_end' events prior.  If 'p' option is unspecified, it defaults to 1 (i.e., copy only the previous one value of <field> into <field>_p1.  The first few events will lack previous values, since they do not exist.",
      "example1": "... | eval rawlen=len(_raw) | autoregress rawlen p=1-4 | eval moving_average = (rawlen + rawlen_p1 + rawlen_p2 + rawlen_p3 + rawlen_p4) / 5",
      "example2": "... | autoregress count p=2-5",
      "example3": "... | autoregress foo AS oldfoo p=3",
      "related": "accum, delta, streamstats, trendline",
      "shortdesc": "Prepares events or results for calculating the moving average.",
      "syntax": "autoregress <field> (AS <field:newfield>)? (p=<int:p_start>(\"-\"<int:p_end>)?)?",
      "tags": "average mean",
      "usage": "public"
   },
   "bin": {
      "alias": "bucket, discretize",
      "category": "reporting",
      "commentcheat1": "Separate search results into 10 bins, and return the count of raw events for each bin.",
      "commentcheat2": "Return the average \"thruput\" of each \"host\" for each 5 minute time span.",
      "description": "Puts continuous numerical field values into discrete sets, or bins. Adjusts the value of 'field', so that all items in the set have the same value for 'field'.  Note: Bin is called by chart and timechart automatically and is only needed for statistical operations that timechart and chart cannot process.",
      "examplecheat1": "... | bin size bins=10 | stats count(_raw) by size",
      "examplecheat2": "... | bin _time span=5m | stats avg(thruput) by _time host",
      "related": "chart, timechart",
      "shortdesc": "Puts continuous numerical values into discrete sets.",
      "syntax": "bin (<bin-options> )* <field> (as <field>)?",
      "tags": "bucket band bracket bin round chunk lump span",
      "usage": "public"
   },
   "bucketdir": {
      "category": "results::group",
      "comment1": "get 10 best sources and directories",
      "description": "Returns at most MAXCOUNT events by taking the incoming events and rolling up multiple sources into directories, by preferring directories that have many files but few events.  The field with the path is PATHFIELD (e.g., source), and strings are broken up by a SEP character.  The default pathfield=source; sizefield=totalCount; maxcount=20; countfield=totalCount; sep=\"/\" or \"\\\\\", depending on the os.",
      "example1": "... | top source|bucketdir pathfield=source sizefield=count maxcount=10",
      "related": "cluster dedup",
      "shortdesc": "Replaces PATHFIELD with higher-level grouping, such as replacing filenames with directories.",
      "syntax": "bucketdir pathfield=<field> sizefield=<field> (maxcount=<int>)? (countfield=<field>)? (sep=<char>)?",
      "tags": "cluster group collect gather",
      "usage": "public"
   },
   "chart": {
      "category": "reporting",
      "commentcheat1": "Return the average (mean) \"size\" for each distinct \"host\".",
      "commentcheat2": "Return the the maximum \"delay\" by \"size\", where \"size\" is broken down into a maximum of 10 equal sized buckets.",
      "commentcheat3": "Return the ratio of the average (mean) \"size\" to the maximum \"delay\" for each distinct \"host\" and \"user\" pair.",
      "commentcheat4": "Return max(delay) for each value of foo split by the value of bar.",
      "commentcheat5": "Return max(delay) for each value of foo.",
      "description": "Creates a table of statistics suitable for charting.  Whereas timechart generates a\n             chart with _time as the x-axis, chart lets you select an arbitrary field as the\n             x-axis with the \"by\" or \"over\" keyword. If necessary, the x-axis field is converted\n             to discrete numerical quantities.\\p\\\n             When chart includes a split-by-clause, the columns in the output table represents a\n             distinct value of the split-by-field. (With stats, each row represents a single\n             unique combination of values of the group-by-field. The table displays ten columns\n             by default, but you can specify a where clause to adjust the number of columns.\\p\\\n             When a where clause is not provided, you can use limit and agg options to specify\n             series filtering. If limit=0, there is no series filtering. \\p\\\n             When specifying multiple data series with a split-by-clause, you can use sep and\n             format options to construct output field names.",
      "examplecheat1": "... | chart avg(size) by host",
      "examplecheat2": "... | chart max(delay) by size bins=10",
      "examplecheat3": "... | chart eval(avg(size)/max(delay)) by host user",
      "examplecheat4": "... | chart max(delay) over foo by bar",
      "examplecheat5": "... | chart max(delay) over foo",
      "related": "timechart, bucket, sichart",
      "shortdesc": "Returns results in a tabular output for charting.",
      "simplesyntax": "chart (agg=<stats-agg-term>)? ( <stats-agg-term> | ( \"(\" <eval-expression> \")\" ) )+ ( BY <field> (<bin-options> )* (<split-by-clause>)? )? | ( OVER <field> (<bin-options>)* (BY <split-by-clause>)? )?",
      "supports-multivalue": "true",
      "syntax": "chart <chart-command-arguments>",
      "tags": "chart graph report sparkline count dc mean avg stdev var min max mode median",
      "usage": "public"
   },
   "cluster": {
      "alias": "slc",
      "category": "results::group",
      "comment": "Cluster syslog events together.",
      "commentcheat": "Cluster events together, sort them by their \"cluster_count\" values, and then return the 20 largest clusters (in data size).",
      "description": "Fast and simple clustering method designed to operate on event text (_raw field).  With default options, a single representative event is retained for each cluster.",
      "example": "sourcetype=syslog | cluster",
      "examplecheat": "... | cluster t=0.9 showcount=true | sort - cluster_count | head 20",
      "related": "anomalies, anomalousvalue, cluster, kmeans, outlier",
      "shortdesc": "Clusters similar events together.",
      "syntax": "cluster (<slc-option> )*",
      "tags": "cluster group collect gather",
      "usage": "public"
   },
   "cofilter": {
      "category": "streaming, reporting",
      "comment1": "user field must be specified first and item field second",
      "description": "For this command, we think of field1 values as \"users\" and field2 values as \"items\".\n           The goal of the command is to compute, for each pair of item (i.e., field2 values), how many\n           users (i.e., field1 values) used them both (i.e., occurred with each of them).",
      "example1": "... | cofilter field1 field2",
      "related": "associate, correlate",
      "shortdesc": "Find how many times field1 and field2 values occurred together.",
      "syntax": "cofilter field1 field2",
      "tags": "arules associate contingency correlate correspond dependence independence",
      "usage": "public"
   },
   "collect": {
      "alias": "stash, summaryindex, sumindex",
      "category": "index::summary",
      "comment1": "Put \"download\" events into an index named \"downloadcount\".",
      "description": "Adds the results of the search into the specified index. Behind the scenes, the events are written\n             to a file whose name format is: \"<random-num>_events.stash\", unless overridden, in a directory\n             which is watched for new events by Splunk. If the events contain a _raw field then the raw field\n             is saved, if they don't a _raw field is constructed by concatenating all the fields into a\n             comma-separated list of key=\"value\" pairs.",
      "example1": "eventtypetag=\"download\" | collect index=downloadcount",
      "optout-in": "lite, lite_free",
      "related": "overlap, sichart, sirare, sistats, sitop, sitimechart",
      "shortdesc": "Puts search results into a summary index.",
      "syntax": "collect <collect-index> (<collect-arg>)*",
      "tags": "collect summary overlap summary index summaryindex",
      "usage": "public"
   },
   "concurrency": {
      "category": "reporting",
      "comment1": "Calculate the number of concurrent events for each event start time and emit as field 'foo'",
      "comment2": "Calculate the number of ongoing http requests at the start time of each http request in a splunk access log",
      "commentcheat": "Calculate the number of concurrent events using the 'et' field as the start time\n              and 'length' as the duration.",
      "description": "If each event represents something that occurs over a span of time, where that\n           span is specified in the duration field, calculate the number of concurrent events\n           for each event start time.  An event X is concurrent with event Y if\n           the X start time, X.start, lies between Y.start and (Y.start + Y.duration).\n           In other words, the concurrent set of events is calculated for each event start time,\n           and that number is attached to the event.\n           The units of start and duration are assumed to be the same.  If you have different\n           units, you will need to convert them to corresponding units prior to using the concurrency\n           command.\n           Unless specified, the start field is assumed to be _time and the output field will\n           be 'concurrency'\n           Limits: If concurrency exceeds limits.conf [concurrency] max_count\n           (Defaults to 10 million), results will not be accurate.",
      "example1": "... | concurrency duration=total_time output=foo",
      "example2": "... | eval spent_in_seconds = spent / 1000 | concurrency duration=spent_in_seconds",
      "examplecheat": "... | concurrency duration=length start=et",
      "related": "timechart",
      "shortdesc": "Given a duration field, finds the number of \"concurrent\" events for each event.",
      "syntax": "concurrency duration=<field> (start=<field>)? (output=<field>)?",
      "tags": "concurrency",
      "usage": "public"
   },
   "contingency": {
      "alias": "counttable, ctable",
      "category": "reporting",
      "comment1": "Build a contingency table for fields \"host\" and \"sourcetype\".",
      "commentcheat": "Build a contingency table of \"datafields\" from all events.",
      "description": "In statistics, contingency tables are used to record\n             and analyze the relationship between two or more (usually categorical) variables.  Many metrics of\n             association or independence can be calculated based on contingency tables, such as the phi\n             coefficient or the V of Cramer.",
      "example1": "... | contingency host sourcetype",
      "examplecheat": "... | contingency datafield1 datafield2 maxrows=5 maxcols=5 usetotal=F",
      "related": "associate, correlate",
      "shortdesc": "Builds a contingency table for two fields.",
      "syntax": "contingency (<contingency-option> )* <field> <field>",
      "tags": "associate contingency correlate connect link correspond dependence independence",
      "usage": "public"
   },
   "convert": {
      "category": "fields::convert",
      "commentcheat1": "Convert every field value to a number value except for values in the field \"foo\" (use the \"none\" argument to specify fields to ignore).",
      "commentcheat2": "Change all memory values in the \"virt\" field to Kilobytes.",
      "commentcheat3": "Change the sendmail syslog duration format (D+HH:MM:SS) to seconds. For example, if \"delay=\"00:10:15\"\", the resulting value will be \"delay=\"615\"\".",
      "commentcheat4": "Convert values of the \"duration\" field into number value by removing string values in the field value. For example,  if \"duration=\"212 sec\"\", the resulting value will be \"duration=\"212\"\".",
      "description": "Converts the values of fields into numerical values. When renaming a field using \"as\", the original field is left intact. The timeformat option is used by ctime and mktime conversions.  Default = \"%m/%d/%Y %H:%M:%S\".",
      "examplecheat1": "... | convert auto(*) none(foo)",
      "examplecheat2": "... | convert memk(virt)",
      "examplecheat3": "... | convert dur2sec(delay)",
      "examplecheat4": "... | convert rmunit(duration)",
      "related": "eval",
      "shortdesc": "Converts field values into numerical values.",
      "simplesyntax": "convert (timeformat=<string>)? ( (auto|dur2sec|mstime|memk|none|num|rmunit|rmcomma|ctime|mktime) \"(\" <field>? \")\" (as <field>)?)+",
      "syntax": "convert (timeformat=<string>)? (<convert-function> (as <wc-field>)?)+",
      "tags": "interchange transform translate convert ctime mktime dur2sec mstime memk",
      "usage": "public"
   },
   "correlate": {
      "category": "reporting",
      "comment1": "Calculate the correlation between all fields.",
      "commentcheat": "Calculate the co-occurrence correlation between all fields.",
      "description": "Calculates a co-occurrence matrix, which contains the percentage of times that two\n             fields exist in the same events.  The RowField field contains the name of the field considered\n             for the row, while the other column names (fields) are the fields it is being compared against.\n             Values are the ratio of occurrences when both fields appeared to occurrences when only one field appeared.",
      "example1": "... | correlate",
      "examplecheat": "... | correlate",
      "related": "associate, contingency",
      "shortdesc": "Calculates the correlation between different fields.",
      "syntax": "correlate",
      "tags": "associate contingency correlate connect link correspond dependence independence",
      "usage": "public"
   },
   "datamodel": {
      "category": "results::filter",
      "comment1": "Return JSON for all data models available in the current app context.",
      "comment2": "Return JSON for the internal_server data model.",
      "comment3": "Return JSON for the scheduler dataset within the internal_server\n          data model.",
      "comment4": "Run the search represented by the scheduler dataset within the\n          internal_server data model.",
      "description": "Must be the first command in a search. When used with no\n             arguments, returns the JSON for all data models available in the\n             current context. When used with just a modelName, returns the\n             JSON for a single data model. When used with a modelName and\n             objectName, returns the JSON for a single data model dataset.\n             When used with modelName, objectName and 'dm-search-mode', runs\n             the search for the specified search mode.",
      "example1": "| datamodel",
      "example2": "| datamodel internal_server",
      "example3": "| datamodel internal_server scheduler",
      "example4": "| datamodel internal_server scheduler search",
      "related": "from, pivot",
      "shortdesc": "Allows users to examine data models and search data model datasets.",
      "syntax": "datamodel (<modelName>)? (<objectName>)? (<dm-search-mode>)?",
      "tags": "datamodel model pivot",
      "usage": "public"
   },
   "dbinspect": {
      "category": "administrative",
      "comment1": "Display a chart with the span size of 1 day.",
      "description": "Returns information about the buckets in the Splunk Enterprise index.\n             The Splunk Enterprise index is the repository for data from Splunk Enterprise.\n             As incoming data is indexed, or transformed into events, Splunk Enterprise\n             creates files of rawdata and metadata (index files). The files reside in sets\n             of directories organized by age. These directories are called buckets.\n             When invoked without the bin-span option, information about the buckets\n             is returned in the following fields:\n             bucketId, endEpoch, eventCount, guID, hostCount, id, index, modTime, path,\n             rawSize, sizeOnDiskMB, sourceCount, sourceTypeCount, splunk_server, startEpoch, state,\n             corruptReason. The corruptReason field only appears when corruptonly=true. \\p\\\n             When invoked with a bin span, a table of the spans of each bucket is returned.",
      "example1": "| dbinspect index=_internal span=1d",
      "generating": "true",
      "related": "metadata",
      "shortdesc": "Returns information about the buckets in the Splunk Enterprise index.",
      "syntax": "dbinspect (<index-opt>)* (<bin-span>|<timeformat>)? (corruptonly=<bool>)?",
      "tags": "inspect index bucket",
      "usage": "public beta"
   },
   "dedup": {
      "category": "results::filter",
      "comment1": "For events that have the same 'source' value, keep the first 3 that occur and remove all subsequent events.",
      "comment2": "Remove duplicates of results with the same source value and sort the events by the '_time' field in ascending order.",
      "comment3": "Remove duplicates of results with the same source value and sort the events by the '_size' field in descending order.",
      "commentcheat": "Remove duplicates of results with the same host value.",
      "description": "Keep the first N (where N > 0) results for each combination of values for the specified field(s)\n             The first argument, if a number, is interpreted as N.  If this number is absent, N is assumed to be 1.\n             The optional sortby clause is equivalent to performing a sort command before the dedup command except that it is executed more efficiently.  The keepevents flag will keep all events, but for events with duplicate values, remove those fields values instead of the entire event. \\p\\\n             Normally, events with a null value in any of the fields are dropped.  The keepempty\n             flag will retain all events with a null value in any of the fields.",
      "example1": "... | dedup 3 source",
      "example2": "... | dedup source sortby +_time",
      "example3": "... | dedup group sortby -_size",
      "examplecheat": "... | dedup host",
      "related": "uniq",
      "shortdesc": "Removes events which contain an identical combination of values for selected fields.",
      "syntax": "dedup (<int>)? <field-list> (<dedup-keepevents>)? (<dedup-keepempty>)? (<dedup-consecutive>)? (sortby <sort-by-clause>)?",
      "tags": "duplicate redundant extra",
      "usage": "public beta"
   },
   "delete": {
      "description": "Piping a search to the delete operator marks all the events returned by that search so that they are never returned by any later search. No user (even with admin permissions) will be able to see this data using Splunk.\n             The delete operator can only be accessed by a user with the \"delete_by_keyword\" capability. By default, Splunk ships with a special role, \"can_delete\" that has this capability (and no others). The admin role does not have this capability by default. Splunk recommends you create a special user that you log into when you intend to delete index data.\n             To use the delete operator, run a search that returns the events you want deleted. Make sure that this search ONLY returns events you want to delete, and no other events. Once you've confirmed that this is the data you want to delete, pipe that search to delete.\n             Note: The delete operator will trigger a roll of hot buckets to warm in the affected index(es).",
      "shortdesc": "Deletes (makes irretrievable) events from Splunk indexes.",
      "syntax": "delete",
      "tags": "delete hide",
      "usage": "public"
   },
   "delta": {
      "category": "fields::add",
      "comment1": "For each event where 'count' exists, compute the difference between count and its previous value and store the result in 'countdiff'.",
      "comment2": "Compute the difference between current value of count and the 3rd previous value of count and store the result in 'delta(count)'",
      "description": "For each event where <field> is a number, compute the difference, in search order, between the current event's value of <field> and a previous event's value of <field> and write this difference into <field:newfield>.  If <newfield> if not specified, it defaults to \"delta(<field>)\"   If p is unspecified, the default = 1, meaning the the immediate previous value is used.  p=2 would mean that the value before the previous value is used, etc etc etc.",
      "example1": "... | delta count as countdiff",
      "example2": "... | delta count p=3",
      "note": "Historical search order is from new events to old events, so values ascending over time will show negative deltas, and vice versa.  Realtime search is in the incoming data order, so delta can produce odd values for data which arrives out-of-order from the original data order (eg. when files are acquired out-of-order on forwarders).",
      "related": "accum, autoregress, streamstats, trendline",
      "shortdesc": "Computes the difference in field value between nearby results.",
      "syntax": "delta <field> (as <field:newfield>)? (p=<int>)?",
      "tags": "difference delta change distance",
      "usage": "public"
   },
   "diff": {
      "category": "formatting",
      "comment1": "Compare the 9th search results to the 10th",
      "commentcheat": "Compare the \"ip\" values of the first and third search results.",
      "default": "diff position1=1 position2=2 attribute=_raw header=f context=f",
      "description": "Compares a field from two search results, returning the line-by-line 'diff' of the two.\n             The two search results compared is specified by the two position values (position1 and position2),\n             hich default to 1 and 2 (i.e., compare the first two results).  \\p\\\n             By default, the text of the two search results (i.e., the \"_raw\" field) are compared,\n             but other fields can be compared, using 'attribute'.  \\p\\\n             If 'diffheader' is true, the traditional diff headers are created using the source keys\n             of the two events as filenames. 'diffheader' defaults to false.  \\p\\\n             If 'context' is true, the output is generated in context-diff format.  Otherwise, unified diff format is used.\n             'context' defaults to false (unified). \\p\\\n             If 'maxlen' is provided, it controls the maximum content in bytes diffed from the two events.\n             It defaults to 100000, meaning 100KB, if maxlen=0, there is no limit.",
      "example1": "... | diff position1=9 position2=10",
      "examplecheat": "... | diff pos1=1 pos2=3 attribute=ip",
      "related": "set",
      "shortdesc": "Returns the difference between two search results.",
      "syntax": "diff (position1=<int>)? (position2=<int>)? (attribute=<string>)? (diffheader=<bool>)? (context=<bool>)? (maxlen=<int>)?",
      "tags": "diff differentiate distinguish contrast",
      "usage": "public"
   },
   "erex": {
      "category": "fields::add",
      "comment1": "Extracts out values like \"7/01\", putting them into the \"monthday\" attribute.",
      "comment2": "Extracts out values like \"7/01\" and \"7/02\", but not patterns like \"99/2\", putting extractions into the \"monthday\" attribute.",
      "description": "Example-based regular expression\n  extraction. Automatically extracts field values from FROMFIELD\n  (defaults to _raw) that are similar to the EXAMPLES\n  (comma-separated list of example values) and puts them in FIELD.\n  An informational message is output with the resulting regular expression.\n  That expression can then be used with the REX command for\n  more efficient extraction.  To learn the extraction rule for\n  pulling out example values, it learns from at most MAXTRAINERS\n  (defaults to 100, must be between 1-1000).",
      "example1": "... | erex monthday examples=\"7/01\"",
      "example2": "... | erex monthday examples=\"7/01, 07/02\" counterexamples=\"99/2\"",
      "related": "extract, kvform, multikv, regex, rex, xmlkv",
      "shortdesc": "Automatically extracts field values similar to the example values.",
      "syntax": "erex <field> examples=<erex-examples> (counterexamples=<erex-examples>)? (fromfield=<field>)? (maxtrainers=<int>)?",
      "tags": "regex regular expression extract",
      "usage": "public"
   },
   "eval": {
      "category": "fields::add",
      "comment1": "Set full_name to the concatenation of first_name, a space, and last_name.\nLowercase full_name. An example of multiple eval expressions, separated by a comma.",
      "comment2": "Set sum_of_areas to be the sum of the areas of two circles",
      "comment3": "Set status to some simple http error codes.",
      "comment4": "Set status to OK if error is 200; otherwise, Error.",
      "comment5": "Set lowuser to the lowercase version of username.",
      "commentcheat": "Set velocity to distance / time.",
      "description": "Performs an arbitrary expression evaluation, providing mathematical, string, and boolean operations. The results of eval are written to a specified destination field, which can be a new or existing field. If the destination field exists, the values of the field are replaced by the results of eval. The syntax of the expression is checked before running the search, and an exception will be thrown for an invalid expression. For example, the result of an eval statement is not allowed to be boolean. If search time evaluation of the expression is unsuccessful for a given event, eval erases the value in the result field.",
      "example1": "... | eval full_name = first_name.\" \".last_name, low_name = lower(full_name)",
      "example2": "... | eval sum_of_areas = pi() * pow(radius_a, 2) + pi() * pow(radius_b, 2)",
      "example3": "... | eval error_msg = case(error == 404, \"Not found\", error == 500, \"Internal Server Error\", error == 200, \"OK\")",
      "example4": "... | eval status = if(error == 200, \"OK\", \"Error\")",
      "example5": "... | eval lowuser =  lower(username)",
      "examplecheat": "... | eval velocity=distance/time",
      "related": "where",
      "shortdesc": "Calculates an expression and puts the resulting value into a field. You can specify to calculate more than one expression.",
      "syntax": "eval <eval-field>=<eval-expression> (\",\" <eval-field>=<eval-expression>)*",
      "tags": "evaluate math string bool formula calculate compute abs case cidrmatch coalesce commands exact exp floor if ifnull isbool isint isnotnull isnull isnum isstr len like ln log lower match max md5 min mvappend mvcount mvindex mvfilter mvjoin mvsort mvdedup now null nullif pi pow random relative_time replace round searchmatch sigfig split sqrt strftime strptime substr time tostring trim ltrim rtrim typeof upper urldecode validate",
      "usage": "public"
   },
   "eventcount": {
      "category": "reporting",
      "comment1": "Return the number of events in the '_internal' index.",
      "comment2": "Gives event count by each index/server pair.",
      "comment3": "Displays event count over all search peers.",
      "description": "Returns the number of events in an index.  By default, it summarizes the events across all peers and indexes (summarize is True by default).  If summarize is False, it splits the event count by index and search peer.  If report_size is True (it defaults to False), then it will also report the index size in bytes. If list_vix is False (it defaults to True) then virtual indexes will not be listed.",
      "example1": "| eventcount index=_internal",
      "example2": "| eventcount summarize=false index=*",
      "example3": "| eventcount",
      "shortdesc": "Returns the number of events in an index.",
      "syntax": "eventcount (<index-opt>)* (summarize=<bool>)? (report_size=<bool>)? (list_vix=<bool>)?",
      "tags": "count eventcount",
      "usage": "public"
   },
   "eventstats": {
      "category": "reporting",
      "comment1": "Compute the overall average duration and add 'avgdur' as a new field to each event where the 'duration' field exists",
      "comment2": "Same as example1 except that averages are calculated for each distinct value of date_hour and the aggregate value that is added to each event is the aggregate that perhaps to the value of date_hour in that event.",
      "description": "Generate summary statistics of all existing fields in your search results and save them as values in new fields. Specify a new field name for the statistics results by using the as argument. If you don't specify a new field name, the default field name is the statistical operator and the field it operated on (for example: stat-operator(field)). Just like the 'stats' command except that aggregation results are added inline to each event, and only the aggregations that are pertinent to that event.  The 'allnum' option has the same meaning as that option in the stats command.  See stats-command for detailed descriptions of syntax.",
      "example1": "... | eventstats avg(duration) as avgdur",
      "example2": "... | eventstats avg(duration) as avgdur by date_hour",
      "related": "stats",
      "shortdesc": "Adds summary statistics to all search results.",
      "syntax": "eventstats (allnum=<bool>)? (<stats-agg-term>)* (<by-clause>)?",
      "tags": "stats statistics event",
      "usage": "public"
   },
   "extract": {
      "alias": "kv",
      "category": "fields::add",
      "comment1": "Extract field/value pairs that are defined in the transforms.conf stanza 'access-extractions'.",
      "commentcheat1": "Extract field/value pairs and reload field extraction settings from disk.",
      "commentcheat2": "Extract field/value pairs that are delimited by '|' or ';', and values of fields that are delimited by '=' or ':'.",
      "description": "Forces field-value extraction on the result set.",
      "example1": "... | extract access-extractions",
      "examplecheat1": "... | extract reload=true",
      "examplecheat2": "... | extract pairdelim=\"|;\", kvdelim=\"=:\", auto=f",
      "note": "Use pairdelims & kvdelim to select how to extract data.",
      "related": "kvform, multikv, rex, xmlkv",
      "shortdesc": "Extracts field-value pairs from search results.",
      "syntax": "extract <extract-options>* <extractor-name>*",
      "tags": "extract kv field extract",
      "usage": "public"
   },
   "fieldformat": {
      "category": "formatting",
      "comment1": "Specify that the start_time should be rendered by taking the value of start_time (assuming it is an epoch number) and rendering it to display just the hours minutes and seconds corresponding that epoch time.",
      "description": "Expresses how to render a field at output time without changing the underlying value.",
      "example1": "fieldformat start_time = strftime(start_time, \"%H:%M:%S\")",
      "shortdesc": "Specifies how to display field values.",
      "syntax": "fieldformat <field> = <eval-expression>",
      "tags": "field format",
      "usage": "public"
   },
   "fields": {
      "category": "fields::filter",
      "comment1": "Keep only the fields 'source', 'sourcetype', 'host', and all fields beginning with 'error'.",
      "commentcheat1": "Keep only the \"host\" and \"ip\" fields, and display them in the order: \"host\", \"ip\".",
      "commentcheat2": "Remove the \"host\" and \"ip\" fields.",
      "description": "Keeps or removes fields based on the field list criteria.\n             If \"+\" is specified, only the fields that match one of the fields in the list are kept.\n             If \"-\" is specified, only the fields that match one of the fields in the list are removed.",
      "example1": "... | fields source, sourcetype, host, error*",
      "examplecheat1": "... | fields host, ip",
      "examplecheat2": "... | fields - host, ip",
      "note": "\"_*\" is the wildcard pattern for Splunk internal fields.\n             This is similar to an SQL SELECT statement.",
      "related": "rename",
      "shortdesc": "Keeps or removes fields from search results.",
      "syntax": "fields (\"+\"|\"-\")? <wc-field-list>",
      "tags": "fields select columns",
      "usage": "public"
   },
   "fieldsummary": {
      "category": "reporting",
      "comment1": "Return summaries for all fields",
      "comment2": "Returns summaries for only fields that start with date_ and return only the top 10 values for each field",
      "description": "Generates summary information for all or a subset of the fields.  Emits a maximum of maxvals distinct values for each field (default = 100).",
      "example1": "... | fieldsummary",
      "example2": "... | fieldsummary maxvals=10 date_*",
      "related": "af, anomalies, anomalousvalue, stats",
      "shortdesc": "Generates summary information for all or a subset of the fields.",
      "syntax": "fieldsummary (maxvals=<num>)? <wc-field-list>?",
      "usage": "public"
   },
   "file": {
      "alias": "test",
      "category": "results::read",
      "commentcheat": "Display events from the file \"messages.1\" as if the events were indexed in Splunk.",
      "description": "If filename is a file, the file command will read the file as if it was indexed in Splunk. If filename is a directory, file will display the list of files in that directory with the option of adding those to the inputs.",
      "examplecheat": "| file /var/log/messages.1",
      "related": "inputcsv",
      "shortdesc": "Processes the given file as if it were indexed.",
      "syntax": "file <filename>",
      "tags": "file index read open preview test input",
      "usage": "public"
   },
   "filldown": {
      "category": "fields::modify",
      "comment": "Filldown null values for all fields",
      "comment1": "Filldown null values for the count field only",
      "comment2": "Filldown null values for the count field and any field that starts with 'score'",
      "description": "Replace null values with the last non-null value for a field or set of fields.\n             If no list of fields is given, filldown will be applied to all fields.\n             If there were not any previous values for a field, it will be left blank (null).",
      "example": "... | filldown",
      "example1": "... | filldown count",
      "example2": "... | filldown count score*",
      "related": "fillnull",
      "shortdesc": "Replace null values with the last non-null value.",
      "syntax": "filldown (<wc-field-list>)?",
      "tags": "empty default",
      "usage": "public"
   },
   "fillnull": {
      "category": "fields::modify",
      "comment": "Build a time series chart of web events by host and fill all empty fields with NULL.",
      "comment1": "For the current search results, fill all empty fields with zero.",
      "comment2": "For the current search results, fill all empty fields with NULL.",
      "comment3": "For the current search results, fill all empty field values of \"foo\" and \"bar\" with NULL.",
      "description": "Replaces null values with a user specified value (default \"0\").\n             Null values are those missing in a particular result, but\n             present for some other result.  If a field-list is provided, fillnull\n             is applied to only fields in the given list (including any fields that\n             does not exist at all).  Otherwise, applies to all existing fields.",
      "example": "sourcetype=\"web\" | timechart count by host | fillnull value=NULL",
      "example1": "... | fillnull",
      "example2": "... | fillnull value=NULL",
      "example3": "... | fillnull value=NULL foo bar",
      "related": "eval",
      "shortdesc": "Replaces null values with a specified value.",
      "syntax": "fillnull (value=<string>)? (<field-list>)?",
      "tags": "empty default",
      "usage": "public"
   },
   "findtypes": {
      "category": "results::group",
      "commentcheat": "Discover 50 common event types and add support for looking at text phrases",
      "description": "Takes previous search results, and produces a list of\n promising searches that may be used as event types.  Returns up to MAX\n event types, defaulting to 10.  If the \"notcovered\" keyword is\n specified, then event types that are already covered by other\n eventtypes are not returned.  At most 5000 events are analyzed for\n discovering event types.  If the \"useraw\" keyword is specified, then\n phrases in the _raw text of the events is used for generating event\n types.",
      "examplecheat": "... | findtypes max=50 useraw",
      "note": "replacement for typelearner",
      "related": "typer,typelearner",
      "shortdesc": "Generates suggested event types.",
      "syntax": "findtypes max=<int> notcovered? useraw?",
      "tags": "eventtype typer discover search classify",
      "usage": "public"
   },
   "foreach": {
      "category": "search::subsearch",
      "comment1": "add together all fields with a name that starts with \"test\" into a total field (result should be total=6)",
      "comment2": "for each field that matches foo*, add it to the corresponding bar* field and write to a new_* field (e.g. new_X = fooX + barX)",
      "comment3": "equivalent to:  eval foo=\"foo\" | eval bar=\"bar\" | eval baz=\"baz\"",
      "comment4": "for the field fooAbarX, this would be to equivalent to: eval fooAbarX = \"X\"",
      "description": "Run a templated streaming subsearch for each field in a wildcarded field list.  For each field that is matched, the templated subsearch will have the following patterns replaced:  \\i\\\n    option         default          replacement \\i\\\n    fieldstr       <<FIELD>>        whole field name \\i\\\n    matchstr       <<MATCHSTR>>     part of field name that matches wildcard(s) in the specifier \\i\\\n    matchseg1      <<MATCHSEG1>>    part of field name that matches first wildcard \\i\\\n    matchseg2      <<MATCHSEG2>>    part of field name that matches second wildcard \\i\\\n    matchseg3      <<MATCHSEG3>>    part of field name that matches third wildcard",
      "example1": "... | eval total=0 | eval test1=1 | eval test2=2 | eval test3=3 | foreach test* [eval total=total + <<FIELD>>]",
      "example2": "... | foreach foo* [eval new_<<MATCHSTR>> = <<FIELD>> + bar<<MATCHSTR>>]",
      "example3": "... | foreach foo bar baz [eval <<FIELD>> = \"<<FIELD>>\"]",
      "example4": "... | foreach foo*bar* fieldstr=\"#field#\" matchseg2=\"#matchseg2#\" [eval #field# = \"#matchseg2#\"]",
      "related": "eval",
      "shortdesc": "Run a templatized streaming subsearch for each field in a wildcarded field list.",
      "syntax": "foreach (<wc-field>)+ (fieldstr=<string>)? (matchstr=<string>)? (matchseg1=<string>)? (matchseg2=<string>)? (matchseg3=<string>)? <subsearch>",
      "tags": "subsearch eval computation wildcard fields",
      "usage": "public"
   },
   "format": {
      "category": "search::subsearch",
      "comment1": "Get the top 2 results. Create a search from the host, source and sourcetype fields. Use the specified format values.",
      "[ [ host": "\"mylaptop\" && source=\"syslog.log\" && sourcetype=\"syslog\" ] || [ host=\"bobslaptop\" && source=\"bob-syslog.log\" && sourcetype=\"syslog\" ] ]",
      "comment2": "Get the top 2 results. Create a search from the host, source and sourcetype fields. Use the default format values. The result is a single result in a new field called \"search\":",
      "( ( host": "\"mylaptop\" AND source=\"syslog.log\" AND sourcetype=\"syslog\" ) OR ( host=\"bobslaptop\" AND source=\"bob-syslog.log\" AND sourcetype=\"syslog\" ) )",
      "comment3": "Change the number of results inline with your search by appending the format command to the end of your subsearch.",
      "description": "This command is used implicitly by subsearches. This command takes the results\n             of a subsearch, formats the results into a single result and places that result\n             into a new field called search.\n             The mvsep argument is the separator for multivalue fields. The default\n             separator is OR.\n             The maxresults argument is the maximum number of results to return. The default\n             is 0, which means no limit on the number returned.\n             The six row and column arguments default to: \\\"(\" \\\"(\" \\\"AND\" \")\" \\\"OR\" \\\")\"",
      "example1": "... | head 2 | fields source, sourcetype, host | format \"[\" \"[\" \"&&\" \"]\" \"||\" \"]\"",
      "example2": "... | head 2 | fields source, sourcetype, host | format",
      "example3": "... | format maxresults = <int>",
      "related": "search",
      "shortdesc": "Takes the results of a subsearch and formats them into a single result.",
      "syntax": "format (mvsep=\"<mv separator>\")? (maxresults=<int>)? (<row-prefix> <column-prefix> <column-separator> <column-end> <row-separator> <row end>)?",
      "tags": "format query subsearch",
      "usage": "public"
   },
   "from": {
      "category": "results::filtering",
      "comment1": "Search a built-in data model that is an internal server log for\n          REST API calls.",
      "comment2": "Retrieve data using a saved search.",
      "comment3": "Specify a dataset name that contains spaces.",
      "comment4": "Retrieve data from a lookup file. Search the contents of the KV\n          store collection kvstorecoll that have a CustID value greater\n          than 500 and a CustName value that begins with the letter P.",
      "description": "The from command retrieves data from a named dataset, saved\n             search, report, CSV lookup file, or KV store lookup file. The\n             from command is a generating command and should be the first\n             command in the search. Generating commands use a leading pipe\n             character.",
      "example1": "| from datamodel:\"internal_server.splunkdaccess\"",
      "example2": "| from savedsearch:mysecurityquery",
      "example3": "| from savedsearch:\"Top five sourcetypes\"",
      "example4": "| from inputlookup:kvstorecoll_lookup | where (CustID>500) AND (CustName=\"P*\") | stats count",
      "related": "savedsearch, inputlookup, datamodel",
      "shortdesc": "Retrieves data from a named dataset, saved search, report, or\n           lookup file. Must be the first command in a search.",
      "syntax": "from <dataset-type>:<dataset-name>",
      "tags": "dataset",
      "usage": "public"
   },
   "gauge": {
      "category": "reporting",
      "comment1": "Use the value of the count field as the gauge value and have 4 regions to the gauge (0-25,25-50,50-75,75-100)",
      "description": "Transforms results into a format suitable for display by the Gauge chart types.  Each argument must be a real number or the name of a numeric field.  Numeric field values will be taken from the first input result, the remainder are ignored.  The first argument is the gauge value and is required.  Each argument after that is optional and defines a range for different sections of the gauge.  If there are no range values provided, the gauge will start at 0 and end at 100.  If two or more range values are provided, The gauge will begin at the first range value, and end with the final range value.  Intermediate range values will be used to split the total range into subranges which will be visually distinct.  A single range value is meaningless and will be treated identically as no range values.",
      "example1": "... | gauge count 0 25 50 75 100",
      "related": "eval stats",
      "shortdesc": "Transforms results into a format that can be displayed by the Gauge chart types.",
      "syntax": "gauge (<num>|<field>) ((<num>|<field>)+)?",
      "tags": "stats format display chart dial",
      "usage": "public"
   },
   "gentimes": {
      "category": "results::generate",
      "comment1": "All daily time ranges from oct 25 till today",
      "comment2": "All daily time ranges from 30 days ago until 27 days ago",
      "comment3": "All daily time ranges from oct 1 till oct 5",
      "comment4": "All HOURLY time ranges from oct 1 till oct 5",
      "description": "Generates time range results. This command is useful in conjunction with the 'map' command.",
      "example1": "| gentimes start=10/25/07",
      "example2": "| gentimes start=-30 end=-27",
      "example3": "| gentimes start=10/1/07 end=10/5/07",
      "example4": "| gentimes start=10/1/07 end=10/5/07 increment=1h",
      "generating": "true",
      "related": "map",
      "shortdesc": "Generates time range results.",
      "syntax": "gentimes start=<timestamp> (end=<timestamp>)? (increment=<increment>)?",
      "tags": "time timestamp subsearch range timerange",
      "usage": "public beta"
   },
   "geom": {
      "category": "reporting",
      "comment1": "When no arguments are provided, geom command looks for a column named \"featureCollection\" and a column named \"featureId\" in the event. These commands are present in the default output from a Lookup on the given geoindex.",
      "comment2": "This case specifies spatial index name to \"geo_us_states\".",
      "comment3": "This case specifies featureId to \"state\" field in event.",
      "comment4": "When allFeatures is used, additional rows are appended for each feature that is not already present in the search results.",
      "description": "Geom command can generate polygon geometry in JSON style, for UI visualization. This command depends on lookup having been installed with external_type=geo.",
      "example1": "...| geom",
      "example2": "...| geom \"geo_us_states\"",
      "example3": "...| geom \"geo_us_states\" featureIdField=\"state\"",
      "example4": "...| geom \"geo_us_states\" allFeatures=true",
      "related": "geomfilter, lookup",
      "shortdesc": "Used for choropleth map's UI visualization.",
      "syntax": "geom (<featureCollection>)? (<allFeatures>)? (<featureIdField>)? (gen=<num>)? <min_x>? <min_y>? <max_x>? <max_y>?",
      "tags": "choropleth map",
      "usage": "public"
   },
   "geomfilter": {
      "category": "reporting",
      "comment1": "This case uses the default bounding box, which will clip the whole map.",
      "comment2": "This case clips half of the whole map.",
      "default": "\"min_x=-180 min_y=-90 max_x=180 max_y=90\"",
      "description": "Geomfilter command accepts 2 points that specify a bounding box for clipping choropleth map; points fell out of the bounding box will be filtered out.",
      "example1": "...| geomfilter",
      "example2": "...| geomfilter min_x=-90 min_y=-90 max_x=90 max_y=90",
      "related": "geom",
      "shortdesc": "Geomfilter command is for choropleth map's clipping feature.",
      "syntax": "geomfilter <min_x>? <min_y>? <max_x>? <max_y>?",
      "tags": "choropleth map",
      "usage": "public"
   },
   "geostats": {
      "category": "reporting",
      "comment1": "compute the average rating for each gender after clustering/grouping the events by \"eventlat\" and \"eventlong\" values.",
      "comment2": "cluster events by default latitude and longitude fields \"lat\" and \"lon\" respectively. Calculate the count of such events",
      "comment3": "take events from apache logs, use iplocation to geocode the ip addresses of the client, and then cluster the events based on how many are happening in each hour of the day.",
      "description": "Use the geostats command to compute statistical functions suitable for rendering on\n             a world map. First, the events will be clustered based on latitude and longitude\n             fields in the events.  Then, the statistics will be evaluated on the generated\n             clusters, optionally grouped or split by fields using a by-clause.\\p\\\n             For map rendering and zooming efficiency, geostats generates clustered stats at a\n             variety of zoom levels in one search, the visualization selecting among them. The\n             quantity of zoom levels can be controlled by the options\n             binspanlat/binspanlong/maxzoomlevel. The initial granularity is selected by\n             binspanlat together with binspanlong.  At each level of zoom, the number of bins\n             will be doubled in both dimensions (a total of 4x as many bins for each zoom-in).",
      "example1": "... | geostats latfield=eventlat longfield=eventlong avg(rating) by gender",
      "example2": "... | geostats count",
      "example3": "sourcetype = access_combined_wcookie | iplocation clientip | geostats count by date_hour",
      "related": "stats, xyseries, chart",
      "shortdesc": "Generate statistics which are clustered into geographical bins to be rendered on a world map.",
      "syntax": "geostats (translatetoxy=<bool>)? (latfield=<string>)? (longfield=<string>)? (outputlatfield=<string>)? (outputlongfield=<string>)? (globallimit=<int>)? (locallimit=<int>)? (binspanlat=<float> binspanlong=<float>)? (maxzoomlevel=<int>)? (<stats-agg-term>)* (<by-clause>)?",
      "tags": "stats statistics",
      "usage": "public"
   },
   "head": {
      "category": "results::order",
      "comment1": "Return events until the time span of the data is >= 100 seconds",
      "commentcheat": "Return the first 20 results.",
      "description": "Returns the first n results, or 10 if no integer is specified.\n             New for 4.0, can provide a boolean eval expression, in which case we return events until that expression evaluates to false.",
      "example1": "... | streamstats range(_time) as timerange | head (timerange<100)",
      "examplecheat": "... | head 20",
      "related": "reverse, tail",
      "shortdesc": "Returns the first n number of specified results.",
      "syntax": "head ((<int>)|(\"(\"<eval-expression>\")\"))? (limit=<int>)? (null=<bool>)? (keeplast=<bool>)?",
      "tags": "head first top leading latest",
      "usage": "public beta"
   },
   "highlight": {
      "alias": "hilite",
      "category": "formatting",
      "comment2": "Highlight the text sequence \"access denied\".",
      "commentcheat": "Highlight the terms \"login\" and \"logout\".",
      "description": "Causes each of the space separated or comma-separated strings provided to be highlighted by the splunk web UI.\n             These strings are matched case insensitively.",
      "example2": "... | highlight \"access denied\"",
      "examplecheat": "... | highlight login,logout",
      "related": "iconify, abstract",
      "shortdesc": "Causes UI to highlight selected strings.",
      "simplesyntax": "highlight (<string>)+",
      "syntax": "highlight (<string>)+",
      "tags": "ui search",
      "usage": "public"
   },
   "history": {
      "category": "results::read",
      "comment": "Returns a history of searches as a table",
      "description": "Returns information about searches that the current user has run.\n             By default, the search strings are presented as a field called \"search\".\n             If events=true, then the search strings are presented as the text of the\n             events, as the _raw field.",
      "example": "| history",
      "generating": "true",
      "related": "search",
      "shortdesc": "Returns a history of searches, either as events or as non-event results (default).",
      "syntax": "history (events=<bool>)?",
      "tags": "history search",
      "usage": "public"
   },
   "iconify": {
      "category": "formatting",
      "comment1": "Displays an different icon for each eventtype.",
      "comment2": "Displays an different icon for each process id.",
      "comment3": "Displays an different icon for url and ip combination.",
      "description": "Causes the UI to make a unique icon for each value of the fields listed.",
      "example1": "... | iconify eventtype",
      "example2": "... | iconify pid",
      "example3": "... | iconify url ip",
      "related": "highlight, abstract",
      "syntax": "iconify <field-list>",
      "tags": "ui search icon image",
      "usage": "public"
   },
   "input": {
      "category": "index::add",
      "comment2": "Remove all csv files that are currently being processed",
      "description": "Adds or removes (disables) sources from being processed by splunk, enabling or disabling inputs in inputs.conf, with optional sourcetype and index settings. Any additional attribute=values are set added to inputs.conf.  Changes are logs to $SPLUNK_HOME/var/log/splunk/inputs.log.",
      "example2": "| search source=*csv | input remove",
      "shortdesc": "Adds or disables sources from being processed by Splunk.",
      "syntax": "input (add|remove) (sourcetype=<string>)? (index=<string>)? (<string>=<string>)*",
      "tags": "input index",
      "usage": "public"
   },
   "inputcsv": {
      "category": "results::read",
      "comment1": "Read in events from the CSV file: \"$SPLUNK_HOME/var/run/splunk/csv/foo.csv\".",
      "comment2": "Read in events 101 to 600 from either file 'bar' (if exists) or 'bar.csv'.",
      "comment3": "Same as example1 except that the events are filtered to where foo is greater than 2 or bar equals 5",
      "commentcheat": "Read in results from the CSV file: \"$SPLUNK_HOME/var/run/splunk/csv/all.csv\", keep any that contain the string \"error\", and save the results to the file: \"$SPLUNK_HOME/var/run/splunk/csv/error.csv\"",
      "description": "Populates the results data structure using the given csv file, which is not modified. The filename must refer to a relative path in $SPLUNK_HOME/var/run/splunk/csv (if dispatch option is set to true, filename refers to a file in the job directory in $SPLUNK_HOME/var/run/splunk/dispatch/<job id>/). If the specified file does not exist and the filename did not have an extension, then filename with a \".csv\" extension is assumed. \\i\\\n     The optional argument 'start' controls the 0-based offset of the first event to be read (default=0). The optional argument 'max' controls the maximum number of events to be read from the file (default = 1000000000). 'events' is an option that allows the imported results to be treated as events, i.e., so that a proper timeline and fields picker are displayed. If 'append' is set to true (false by default), the data from the csv file is appended to the current set of results rather than replacing it.",
      "example1": "| inputcsv foo.csv",
      "example2": "| inputcsv start=100 max=500 bar",
      "example3": "| inputcsv foo.csv where foo>2 OR bar=5",
      "examplecheat": "| inputcsv all.csv | search error | outputcsv errors.csv",
      "generating": "true",
      "note": "'keeptempdir' is a debugging option, that if true, retains the temporary directory that the given file is copied into for manipulation by the search pipeline.  This option should not be mentioned in the external documentation or typeahead.",
      "related": "outputcsv",
      "shortdesc": "Loads search results from the specified CSV file.",
      "syntax": "inputcsv (dispatch=<bool>)? (append=<bool>)? (start=<int>)? (max=<int>)? (events=<bool>)? <filename> (WHERE <string:search-query>)?",
      "tags": "input csv load read",
      "usage": "public"
   },
   "inputlookup": {
      "category": "results::read",
      "comment1": "Read in \"users.csv\" lookup file (under $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/*/lookups).",
      "comment2": "Read in \"usertogroup\" lookup table (as specified in transforms.conf).",
      "comment3": "Same as example2 except that the data from the lookup table is appended to any current results.",
      "comment4": "Same as example2 except that the data from the lookup table is filtered to where foo is greater than 2 or bar equals 5 before returned.",
      "comment5": "Read in a geospatial lookup table. This can be used to show all geographic features on a Choropleth map.",
      "description": "Reads in lookup table as specified by a filename (must end with .csv or .csv.gz) or a table name (as specified by a stanza name in transforms.conf).\n  If 'append' is set to true (false by default), the data from the lookup file is appended to the current set of results rather than replacing it.",
      "example1": "| inputlookup users.csv",
      "example2": "| inputlookup usertogroup",
      "example3": "| inputlookup append=t usertogroup",
      "example4": "| inputlookup usertogroup where foo>2 OR bar=5",
      "example5": "| inputlookup geo_us_states",
      "generating": "true",
      "optout-in": "lite, lite_free",
      "related": "inputcsv, join, lookup, outputlookup",
      "shortdesc": "Loads search results from a specified static lookup table.",
      "syntax": "inputlookup (append=<bool>)? (start=<int>)? (max=<int>)? (<filename>|<string:tablename>) (where <string:search-query>)?",
      "tags": "lookup input table",
      "usage": "public"
   },
   "iplocation": {
      "category": "fields::add",
      "commentcheat": "Add location information (based on IP address).",
      "description": "The ip-address field in ip-address-fieldname is looked up in a database and location fields\n             information is added to the event. The fields are City, Continent, Country, MetroCode,\n             Region, Timezone, lat(latitude) and lon(longitude).\n             Not all of the information is available for all ip address ranges, and hence it is\n             normal to have some of the fields empty.\n             The Continent, MetroCode, and Timezone are only added if allfields=true (default is false).\n             prefix=string will add a certain prefix to all fieldnames if you desire to uniquely qualify\n             added field names and avoid name collisions with existing fields (default is NULL/empty string).\n             The lang setting can be used to render strings in alternate languages (for example \"lang=es\"\n             for spanish)  The set of languages depends on the geoip database in use.  The special language\n             \"lang=code\" will return fields as abbreviations where possible.",
      "example1": "sourcetype = access_combined_* | iplocation clientip",
      "example2": "sourcetype = access_combined_* | iplocation allfields=true clientip",
      "example3": "sourcetype = access_combined_* | iplocation prefix=iploc_ allfields=true clientip",
      "examplecheat": "... | iplocation clientip",
      "shortdesc": "Extracts location information from IP addresses using 3rd-party databases.",
      "syntax": "iplocation (prefix=<string>)? (allfields=<bool>)? (lang=<string>)? <ip-address-fieldname>",
      "tags": "ip location city geocode",
      "usage": "public"
   },
   "join": {
      "category": "results::append",
      "comment1": "Joins previous result set with results from 'search vendors', on\n          the product_id field common to both result sets.",
      "comment2": "Joins previous result set with results from 'search vendors', on\n          the product_id field forced to be common to both result sets.",
      "comment3": "Joins previous result set with results from 'search vendors', on\n          the product id field represented by field names that do match in\n          the two result sets.",
      "comment4": "Joins previous result set with results from a built-in data model\n          that is an internal server log for REST API calls.",
      "description": "You can perform an inner or left join. Use either 'outer' or\n             'left' to specify a left outer join. One or more of the fields\n             must be common to each result set. If no fields are specified,\n             all of the fields that are common to both result sets are used.\n             Limitations on the join subsearch are specified in the\n             limits.conf.spec file. Note: Another command, such as append or\n             lookup, in combination with either stats or transaction might\n             be a better alternative to the join command for flexibility and\n             performance. \\p\\\n             The arguments 'left' and 'right' allow for specifying aliases\n             in order to preserve the lineage of the fields in both result\n             sets. The 'where' argument specifies the aliased fields to join\n             on, where the fields are no longer required to be common to both\n             result sets. \\p\\",
      "example1": "... | join product_id [search vendors]",
      "example2": "... | join product_id [search vendors | rename pid AS product_id]",
      "example3": "... | join left=L right=R WHERE L.product_id=R.pid [search vendors]",
      "example4": "... | join datamodel:\"internal_server.splunkdaccess\"",
      "related": "append, lookup, appendcols, lookup, selfjoin, transaction",
      "shortdesc": "Use to combine the results of a subsearch with the results of a\n           main search.",
      "syntax": "join (<join-options>)* <join-constraints> <dataset>",
      "tags": "join combine unite append csv lookup inner outer left",
      "usage": "public"
   },
   "kmeans": {
      "category": "results::group",
      "comment1": "Group results into 2 clusters based on the values of all numerical fields.",
      "commentcheat": "Group search results into 4 clusters based on the values of the \"date_hour\" and \"date_minute\" fields.",
      "description": "Performs k-means clustering on select fields (or all numerical fields if empty).  Events in the same cluster are\n             moved next to each other.  You have the option to display the cluster number for each event. The centroid of each cluster is also\n             be displayed (with an option to disable it).",
      "example1": "... | kmeans",
      "examplecheat": "... | kmeans k=4 date_hour date_minute",
      "related": "anomalies, anomalousvalue, cluster, outlier",
      "shortdesc": "Performs k-means clustering on selected fields.",
      "syntax": "kmeans (<kmeans-options> )* (<field-list>)?",
      "tags": "cluster group collect gather",
      "usage": "public"
   },
   "kvform": {
      "category": "fields::add",
      "comment1": "Extract values from \"eventtype.form\" if the file exists.",
      "description": "Extracts key/value pairs from events based on a form\n  template that describes how to extract the values.  If FORM is specified,\n  it uses an installed <FORM>.form file found in the splunk configuration form directory.\n  For example, if \"form=sales_order\", would look for a \"sales_order.form\"\n  file in the 'forms' subdirectory in all apps, e.g. $SPLUNK_HOME$/etc/apps/*/forms/.\n  All the events processed would\n  be matched against that form, trying to extract values.\\p\\\n  If no FORM is specified, then the FIELD value determines the name of the field to\n  extract.  For example, if \"field=error_code\", then an event that has an error_code=404,\n  would be matched against a \"404.form\" file.\\p\\\n  The default value for FIELD is \"sourcetype\", thus by default kvform will look for\n  <SOURCETYPE>.form files to extract values.\\p\\\n  A .form file is essentially a text file or all static parts of a form,\n  interspersed with named references to regular expressions, of the type found in\n  transforms.conf.  A .form might might look like this:\\i\\\n             Students Name: [[string:student_name]] \\i\\\n             Age: [[int:age]] Zip: [[int:zip]] .",
      "example1": "... | kvform field=eventtype",
      "note": "Multiple whitespaces, including blanklines are removed from matching, which could be merged into kv to be called automatically, having no cost if the needed .form file is not found.",
      "related": "extract, multikv, rex, xmlkv",
      "shortdesc": "Extracts values from search results, using a form template.",
      "syntax": "kvform (form=<string>)? (field=<field>)?",
      "tags": "form extract template",
      "usage": "public/experimental"
   },
   "loadjob": {
      "category": "results::generate",
      "comment1": "Loads the events that were generated by the search job with id=1233886270.2",
      "comment2": "Loads the results of the latest scheduled execution of savedsearch MySavedSearch in the 'search' application owned by admin",
      "description": "The artifacts to load are identified either by the search job id or a scheduled search name and the time range of the current search. If a savedsearch name is provided and multiple artifacts are found within that range the latest artifacts are loaded.",
      "example1": "| loadjob 1233886270.2 events=t",
      "example2": "| loadjob savedsearch=\"admin:search:MySavedSearch\"",
      "generating": "true",
      "related": "inputcsv, file",
      "shortdesc": "Loads events or results of a previously completed search job.",
      "syntax": "loadjob (<sid-opt>|<savedsearch-identifier>) <result-event-opt>? <delegate-opt>? <artifact-offset-opt>? <ignore-running-opt>?",
      "tags": "artifacts",
      "usage": "public"
   },
   "localize": {
      "category": "search::subsearch",
      "comment1": "As an example, searching for \"error\" and then calling localize finds good regions around\n          where error occurs, and passes each on to the search inside of the the map command, so\n          that each iteration works with a specific timerange to find promising transactions",
      "commentcheat": "Search the time range of each previous result for \"failure\".",
      "description": "Generates a list of time contiguous event regions\n             defined as: a period of time in which consecutive events\n             are separated by at most 'maxpause' time. The found regions\n             can be expanded using the 'timeafter' and 'timebefore' modifiers\n             to expand the range after/before the last/first event in\n             the region respectively. The Regions are return in time descending\n             order, just as search results (time of region is start time).\n             The regions discovered by localize are meant to be feed into\n             the MAP command, which will use a different region for each iteration.\n             Localize also reports: (a) number of events in the range, (b) range\n             duration in seconds and (c) region density defined as (#of events in range)\n             divided by (range duration) - events per second.",
      "example1": "error | localize | map search=\"search starttimeu::$starttime$ endtimeu::$endtime$ |transaction uid,qid maxspan=1h\"",
      "examplecheat": "... | localize maxpause=5m | map search=\"search failure starttimeu=$starttime$ endtimeu=$endtime$\"",
      "related": "map, transaction",
      "shortdesc": "Returns a list of time ranges in which the search results were found.",
      "syntax": "localize <lmaxpause-opt>? <after-opt>? <before-opt>?",
      "tags": "time timestamp subsearch range timerange",
      "usage": "public beta"
   },
   "localop": {
      "category": "search::search",
      "comment1": "The iplocation command in this case will never be run on remote peers.  All events from remote peers from the initial search for the terms FOO and BAR will be forwarded to the search head where the iplocation command will be run.",
      "description": "Prevents subsequent commands from being executed on remote peers, i.e. forces subsequent commands to be part of the reduce step.",
      "example1": "FOO BAR | localop | iplocation clientip",
      "optout-in": "lite, lite_free",
      "shortdesc": "Prevents subsequent commands from being executed on remote peers.",
      "syntax": "localop",
      "tags": "debug distributed",
      "usage": "public unsupported/beta"
   },
   "lookup": {
      "category": "fields::read",
      "comment1": "There is a lookup table specified in a stanza name 'usertogroup' in transform.conf.  This lookup table contains (at least) two fields, 'user' and 'group'. For each event, we look up the value of the field 'local_user' in the table and for any entries that matches, the value of the 'group' field in the lookup table will be written to the field 'user_group' in the event.",
      "description": "Manually invokes field value lookups from an existing lookup table or external\n             script. Lookup tables must be located in the lookups directory of\n             $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/<app-name>/lookups.\n             External scripts must be located in $SPLUNK_HOME/etc/searchscripts or\n             $SPLUNK_HOME/etc/apps/<app_name>/bin.\\p\\\n             Specify a lookup field to match to a field in the events and, optionally,\n             destination fields to add to the events. If you do not specify destination fields,\n             adds all fields in the lookup table to events that have the match field. You can\n             also overwrite fields in the events with fields in the lookup table, if they have\n             the same field name.",
      "example1": "... | lookup usertogroup user as local_user OUTPUT group as user_group",
      "related": "appendcols inputlookup outputlookup",
      "shortdesc": "Explicitly invokes field value lookups.",
      "syntax": "lookup (local=<bool>)? (update=<bool>)? (event_time_field=<string>)? <string:lookup-table-name> (<field:lookup> (as <field:local>)? )+ (OUTPUT|OUTPUTNEW (<field:dest> (as <field:local-dest>)? )+ )?",
      "tags": "join combine append lookup table",
      "usage": "public"
   },
   "makecontinuous": {
      "category": "reporting",
      "comment1": "Make \"_time\" continuous with a span of 10 minutes.",
      "description": "Makes a field that is supposed to be the x-axis continuous (invoked by chart/timechart).",
      "example1": "... | makecontinuous _time span=10m",
      "related": "chart timechart",
      "syntax": "makecontinuous (<field>)? (<bin-options>)*",
      "tags": "continuous",
      "usage": "public"
   },
   "makejson": {
      "category": "results::filter",
      "comment1": "Create a stringified JSON: { \"name\": \"<value of name>\", \"data\": { \"count\": <value of data.count>, \"metrics\": [values of data.metrics] }}",
      "description": "Combines the specified set of field names, or field name patterns,\n\t\t\tand creates an field with the output name.",
      "example1": "... | makejson name data.* \"counts[float]\" output=json_event",
      "shortdesc": "Combines specified fields into a stringified JSON",
      "syntax": "makejson <wc-field-list> (output=<string>)",
      "tags": "json",
      "usage": "public"
   },
   "makemv": {
      "category": "fields::convert",
      "comment1": "Separate the value of \"foo\" into multiple values.",
      "comment2": "For sendmail search results, separate the values of \"senders\" into multiple values. Then, display the top values.",
      "description": "Treat specified field as multi-valued, using either a simple string delimiter (can be multicharacter), or a regex tokenizer.  If neither is provided, a default delimiter of \" \" (single space) is assumed.\n             The allowempty=<bool> option controls if consecutive delimiters should be treated as one (default = false).\n             The setsv boolean option controls if the original value of the field should be kept for the single valued version.  It is kept if setsv = false, and it is false by default.",
      "example1": "... | makemv delim=\":\" allowempty=t foo",
      "example2": "eventtype=\"sendmail\" | makemv delim=\",\" senders | top senders",
      "related": "mvcombine, mvexpand, nomv",
      "shortdesc": "Changes a specified field into a multi-value field during a search.",
      "syntax": "makemv (delim=<string> |tokenizer=<string>)? (allowempty=<bool>)? (setsv=<bool>)? <field>",
      "tags": "multivalue convert",
      "usage": "public"
   },
   "makeresults": {
      "category": "results::generate",
      "description": "Creates a specified number of empty search results. This command will run only on the local machine\nby default and will generate one unannotated empty result. It maybe used in conjunction with the eval command to \ngenerate an empty result for the eval command to operate on.",
      "example1": "makeresults | eval foo=\"foo\"",
      "example2": "index=_internal _indextime > [makeresults | eval it=now()-60 | return $it]",
      "note": "If the search begins with an eval command it will return no results. makeresults is implicitly injected to the\nbeginning of such searches.",
      "shortdesc": "Create a specified number of empty results.",
      "syntax": "makeresults (<count-option>)? (<annotate-option>)? (<splunk-server-option>)? (<splunk-server-group-option>)*",
      "usage": "public"
   },
   "map": {
      "category": "results::generate",
      "description": "For each input search result, takes the field-values\nfrom that result and substitutes their value for the $variable$ in the\nsearch argument.  The value of variables surrounded in quotes (e.g. text=\"$_raw$\") will be quote escaped.\nThe search argument can either be a subsearch to run\nor just the name of a savedsearch. The following metavariables are\nalso supported:\n1. $_serial_id$ - 1-based serial number within map of the search being executed.",
      "example1": "error | localize | map mytimebased_savedsearch",
      "related": "gentimes, search",
      "shortdesc": "Looping operator, performs a search over each search result.",
      "syntax": "map (<searchoption>|<savedsplunkoption>) <maxsearchesoption>?",
      "tags": "map subsearch loop savedsearch",
      "usage": "public beta"
   },
   "mcollect": {
      "category": "index::summary",
      "comment1": "Generate a count of error events as metric data points.",
      "description": "Converts search results into metric data and inserts the data into a metric index\n             on the search head. If each result contains only one metric_name field\n             and one numeric _value field, the result is already a normalized metrics data point,\n             the result does not need to be split and can be consumed directly.\n             Otherwise, each result is spit into multiple metric data points based on the specified\n             list of dimension fields.\n             If the '_time' field is present in the results, it is used as the timestamp of the\n             metric datapoint. If the '_time' field is not present, the current time is used.\n             Arguments:\n             index: The index where the collected metric data are placed. This argument is required.\n             file: The file name where you want the collected metrics data to be written.\n             The default file name is a random filename. You can use a timestamp or a random number\n             for the file name by specifying either file=$timestamp$ or file=$random$.\n             Defaults to $random$_metrics.csv\n             split: If split=false (which is the default setting), the results must include a\n             'metric_name' field for the name of the metric, and a '_value' field for the\n             numerical value of the metric. If split=true, <field-list> must be specified.\n             spool: If spool=true (which is the default setting), the metrics data file is written\n             to the Splunk spool directory, $SPLUNK_HOME/var/spool/splunk, where the file is indexed\n             automatically. If spool=false, the file is written to the $SPLUNK_HOME/var/run/splunk\n             directory. The file remains in this directory unless some form of further automation\n             or administration is done. \\",
      "example1": "ERROR | stats count BY type | rename count AS _value type AS metric_name | mcollect index=my_metric_index",
      "optout-in": "lite, lite_free",
      "related": "collect meventcollect",
      "shortdesc": "Puts search results into a metric index on the search head.",
      "syntax": "mcollect (index=<string>) (file=<string>)? (split=<bool>)? (spool=<bool>)? (prefix_field=<string>)? (host=<string>)? (source=<string>)? (sourcetype=<string>)? (<field-list>)?",
      "tags": "collect summary summaryindex metrics",
      "prefix_field: Is applicable only when split": "true. If specified, any event with that\n             field missing is ignored. Otherwise, the field value is prefixed to the metric name.\n             \"host\": The name of the host that you want to specify for the collected metrics data.\n             Only applicable when spool=true.\n             \"source\": The name of the source that you want to specify for the collected metrics data.\n             Defaults to the name of search.\n             \"sourcetype\": The name of the source type that is specified for the collected metrics\n             data. This setting defaults to mcollect_stash. License usage is not calculated for\n             data indexed with the mcollect_stash source type. If you change to a different\n             source type, the Splunk platform calculates license usage for any data indexed\n             by the mcollect command. NOTE: Do not change this setting without\n             assistance from Splunk Professional Services or Splunk Support. Changing the\n             source type requires a change to the props.conf file.\n             field-list: A list of dimension fields. Optional if split=false (the default), required\n             if split=true. If field-list is not specified, all fields are treated as dimensions\n             for the data point except for the prefix_field and internal fields (fields with an\n             underscore _ prefix). If field-list is specified, the list must be specified\n             at the end of the mcollect command arguments.  If field-list is specified, all\n             fields are treated as metric values, except for fields in field-list,\n             the prefix-field, and internal fields.\n             The name of each metric value is the field name prefixed with the prefix_field value.\n             Effectively, one metric data point is returned for each qualifying field that\n             contains a numerical value. If one search result contains multiple qualifying\n             metric name/value pairs, the result is split into multiple metric data points.",
      "usage": "public"
   },
   "metadata": {
      "category": "administrative",
      "comment1": "Return the values of \"host\" for events in the \"_internal\" index.",
      "comment2": "Return values of \"sourcetype\" for events in the \"_audit\" index on server peer01",
      "comment3": "Return values of \"sourcetype\" for events in the \"_audit\" index on any server name that begins with \"peer\".",
      "comment4": "Return the values of \"host\" for data points in the \"mymetrics\" index.",
      "comment5": "Return the values of \"source\" for data points in all metrics indexes.",
      "description": "This search command generates a list of source, sourcetypes, or hosts from the index. Optional splunk_server argument specifies whether or not to limit results to one specific server. Optional datatype argument specifies whether to only search from event indexes or metrics index. If datatype is not specified, only search from event indexes.",
      "example1": "| metadata type=hosts index=_internal",
      "example2": "| metadata type=sourcetypes index=_audit splunk_server=peer01",
      "example3": "| metadata type=sourcetypes index=_audit splunk_server=peer*",
      "example4": "| metadata type=hosts index=mymetrics datatype=metric",
      "example5": "| metadata type=sources index=* datatype=metric",
      "related": "dbinspect",
      "shortdesc": "Returns a list of source, sourcetypes, or hosts.",
      "syntax": "metadata type=<metadata-type> (<index-opt>)* (splunk_server=<wc-string>)?  (splunk_server_group=<wc-string>)* (datatype=<metric|event>)?",
      "tags": "metadata host source sourcetype metric",
      "usage": "public"
   },
   "metasearch": {
      "category": "search::search",
      "comment1": "Return metadata for events with \"404\" and from host \"webserver1\"",
      "description": "Retrieves event metadata from indexes based on terms in the <logical-expression>.  Metadata fields include source, sourcetype, host, _time, index, and splunk_server.",
      "example1": "404 host=\"webserver1\"",
      "related": "search metadata",
      "shortdesc": "Retrieves event metadata from indexes based on terms in the <logical-expression>.",
      "simplesyntax": "metasearch <logical-expression>?",
      "syntax": "metasearch <logical-expression>?",
      "tags": "search query find",
      "usage": "public"
   },
   "meventcollect": {
      "category": "index::summary",
      "comment1": "collect metrics.log data into a metrics index",
      "description": "Converts search results into metric data and inserts the data into a metric index\n             on the indexers. If each result contains only one metric_name field and one\n             numeric _value field, the result is already a normalized metrics data point,\n             the result does not need to be split and can be consumed directly.\n             Otherwise, each result is spit into multiple metric data points based on the specified\n             list of dimension fields.\n             Only purely streaming commands can precede the meventcollect command so that results can be\n             directly ingested on the indexers.\n             Arguments:\n             index: The index where the collect metric data are placed. This argument is required.\n             split: If split=false (which is the default setting), the results must include a\n             'metric_name' field for the name of the metric, and a '_value' field for the\n             numerical value of the metric. If split=true, <field-list> must be specified.\n             spool: If spool=true (which is the default setting), the metrics data file is written\n             to the Splunk spool directory, $SPLUNK_HOME/var/spool/splunk, where the file is indexed\n             automatically. If spool=false, the file is written to the $SPLUNK_HOME/var/run/splunk\n             directory. The file remains in this directory unless some form of further automation\n             or administration is done. \\",
      "example1": "index=_internal source=*/metrics.log | eval prefix = group + \".\" + name | meventcollect index=my_metric_index split=true prefix_field=prefix name group",
      "optout-in": "lite, lite_free",
      "related": "collect mcollect",
      "shortdesc": "Puts search results into a metric index on the indexers.",
      "syntax": "meventcollect (index=<string>) (split=<bool>)? (spool=<bool>)? (prefix_field=<string>)? (host=<string>)? (source=<string>)? (sourcetype=<string>)? (<field-list>)?",
      "tags": "collect summary summaryindex metrics",
      "prefix_field: Is applicable only when split": "true. If specified, any event with that\n             field missing is ignored. Otherwise, the field value is prefixed to the metric name.\n             \"host\": The name of the host that you want to specify for the collected metrics data.\n             Only applicable when spool=true.\n             \"source\": The name of the source that you want to specify for the collected metrics data.\n             Defaults to the name of search.\n             \"sourcetype\": The name of the source type that is specified for the collected metrics\n             data. This setting defaults to mcollect_stash. License usage is not calculated for\n             data indexed with the mcollect_stash source type. If you change to a different\n             source type, the Splunk platform calculates license usage for any data indexed\n             by the meventcollect command. NOTE: Do not change this setting\n             without assistance from Splunk Professional Services or Splunk Support. Changing\n             the source type requires a change to the props.conf file.\n             field-list: A list of dimension fields. Optional if split=false (the default), required\n             if split=true. If field-list is not specified, all fields are treated as dimensions\n             for the data point except for the prefix_field and internal fields (fields with an\n             underscore _ prefix). If field-list is specified, the list must be specified\n             at the end of the mcollect command arguments.  If field-list is specified, all\n             fields are treated as metric values, except for fields in field-list,\n             the prefix-field, and internal fields.\n             The name of each metric value is the field name prefixed with the prefix_field value.\n             Effectively, one metric data point is returned for each qualifying field that\n             contains a numerical value. If one search result contains multiple qualifying\n             metric name/value pairs, the result is split into multiple metric data points.",
      "usage": "public"
   },
   "mstats": {
      "category": "reporting",
      "comment1": "Get the count of all measurements in the \"mymetrics\" index where\n          the metric name is \"foo\".",
      "comment2": "Get the count and average of all measurements in the \"mymetrics\"\n          index where the metric_name value is \"foo\".",
      "comment3": "Get the count of all measurements in the \"mymetrics\" index where\n          the metric_name value is \"foo\" and the average of all measurements\n          in the \"mymetrics\" index where the metric_name value is \"bar\".",
      "comment4": "Return the average of all measurements in mymetrics, where bar is\n          \"value2\" and metric_name is \"foo\".",
      "comment5": "Get the count by app for data points with host \"x\" and\n          metric_name \"foo\" from the default metrics indexes.",
      "comment6": "Get a timechart of all the data in your metrics indexes for\n          metric_name \"foo\" with a granularity of one day.",
      "comment7": "Get the median of all measurements from the default metrics indexes,\n          where metric name is \"foo\".",
      "comment8": "Alternative syntax for getting the count of all measurements in\n          the \"mymetrics\" index for a metric_name called \"foo\".",
      "comment9": "Get the average of all values of a metric named either \"cpu.util\" or \"cpu.utilization\".",
      "description": "Performs statistics on the measurement, metric_name, and\n             dimension fields in metric indexes. The mstats command is\n             optimized for searches over one or more metric_name values,\n             rather than searches over all metric_name values. It supports\n             both historical and real-time searches. For a real-time search\n             with a time window, mstats runs a historical search first that\n             backfills the data.\\p\\\n             The mstats command is a generating command, except when it is\n             in 'append=t' mode. As such, it must be the first command in a\n             search.\\p\\\n      If the <stats-func> based syntax is used, the filter specified after\n      the WHERE clause cannot filter on metric_name. Any metric_name\n      filtering is performed based on the metric_name fields specified\n      by the <stats-func> argument.  If the <stats-func-value> syntax\n      is used, the WHERE clause *must* filter on metric_name (wildcards are ok).\n             It is recommended to use the <stats-func> syntax when possible.\n      The <stats-func-value> syntax is needed for cases where a single metric may be\n             represented by several different metric names (e.g. \"cpu.util\" and \"cpu.utilization\"). \\p\\\n             You cannot blend the <stats-func> syntax with the <stats-func-value> syntax in a single mstats command. \\p\\\n             Arguments: \\p\\\n             \"<stats-func>\": A list of stats functions to compute for given\n                       metric_names. These are written as\n                       <function1>(metric_name1) <function2>(metric_name2) ... \\p\\\n             \"<stats-func-value>\": A list of stats functions to compute on\n                metric values (_value).  These are written as\n                       <function1>(_value) <function2>(_value) ... \\p\\\n             \"<logical-expression>\": An expression describing the filters\n                       that are applied to your search. Includes time and\n                       search modifiers, comparison expressions, and index\n                       expressions. This expression cannot filter on\n                       metric_name if the <stats-func> syntax is used, but\n\t\tmust filter on metric_name if the <stats-func-value>\n\t\tsyntax is used.\\p\\\n             \"<field-list>\": Specifies one or more fields to group the\n                       results by. Required when using the 'BY' or\n                       'GROUPBY' clause. \\p\\\n             \"prestats\": Returns the results in prestats format. You can pipe\n                       the results into commands that consume the prestats\n                       formatted data, such as chart or timechart, and\n                       output aggregate calculations. This is useful for\n                       creating graphs. Default is prestats=false. \\p\\\n             \"append\": Valid only when \"prestats=true\". This argument adds\n                       the results of the mstats run to an existing set of\n                       results instead of generating new results. Default is\n                       \"append=false\". \\p\\\n             \"backfill\": Valid only with windowed real-time searches. When\n                       set to \"true\", the mstats command runs a historical\n                       search to backfill the on-disk indexed data before\n                       searching the in-memory real-time data. Default is\n                       \"backfill=true\".\\p\\\n             \"update_period\": Valid only with real-time searches. Specifies\n\t\t            how frequently, in milliseconds, the real-time\n                       summary for the mstats command is updated. By\n                       default, update_period=0, which is 1 second. A\n                       larger number means less frequent reads of the\n                       summary and less impact on index processing.",
      "example1": "| mstats count(foo) WHERE index=mymetrics",
      "example2": "| mstats count(foo) avg(foo) WHERE index=mymetrics",
      "example3": "| mstats count(foo) avg(bar) WHERE index=mymetrics",
      "example4": "| mstats count(foo) WHERE index=mymetrics bar=value2",
      "example5": "| mstats count(foo) WHERE host=x BY app",
      "example6": "| mstats prestats=t count(foo) span=1d | timechart span=1d count(foo)",
      "example7": "| mstats median(foo)",
      "example8": "| mstats count(_value) WHERE metric_name=foo AND index=mymetrics",
      "example9": "| mstats avg(_value) WHERE metric_name=cpu.util OR metric_name=cpu.utilization AND index=mymetrics",
      "optout-in": "lite, lite_free",
      "related": "tstats",
      "shortdesc": "Performs statistics on the measurement, metric_name and dimension fields in metric indexes. Supports historical and real-time search.",
      "syntax": "mstats (prestats=<bool>)? (append=<bool>)? (backfill=<bool>)? (update_period=<int>)? ((<stats-func>)+|(<stats-func-value>)+) WHERE (<logical-expression>)* ((BY|GROUPBY) <field-list> (span=<string:timespan>)? )?",
      "tags": "mstats metric tsidx projection",
      "usage": "public"
   },
   "multikv": {
      "category": "fields::add",
      "comment1": "Extract the \"pid\" and \"command\" fields.",
      "commentcheat": "Extract the \"COMMAND\" field when it occurs in rows that contain \"splunkd\".",
      "description": "Extracts fields from events with information in a tabular format (e.g. top, netstat, ps, ... etc).\n             A new event is created for each table row. Field names are derived from the title row of the table.",
      "example1": "... | multikv fields pid command",
      "examplecheat": "... | multikv fields COMMAND filter splunkd",
      "related": "extract, kvform, rex, xmlkv",
      "shortdesc": "Extracts field-values from table-formatted events.",
      "syntax": "multikv (conf=<stanza_name>)? (<multikv-option>)*",
      "tags": "extract table tabular column",
      "usage": "public"
   },
   "multisearch": {
      "category": "results::append",
      "comment": "search for both events from index a and b and add different fields using eval in each case",
      "description": "Runs multiple *streaming* searches at the same time.  Must specify at least 2 subsearches and only purely streaming operations are allowed in each subsearch (e.g. search, eval, where, fields, rex, ...)",
      "example": "| multisearch [search index=a | eval type = \"foo\"] [search index=b | eval mytype = \"bar\"]",
      "related": "append, join",
      "shortdesc": "Runs multiple searches at the same time.",
      "syntax": "multisearch <subsearch> <subsearch> <subsearch> ...",
      "tags": "append join combine unite combine",
      "usage": "public"
   },
   "mvcombine": {
      "category": "results::filter",
      "comment": "Combine the values of \"foo\" with \":\" delimiter.",
      "description": "For each group of results that are identical except for the given field, combine them into a single result where the given field is a multivalue field.  DELIM controls how values are combined, defaulting to a space character (' ').",
      "example": "... | mvcombine delim=\":\" foo",
      "related": "makemv, mvexpand, nomv",
      "shortdesc": "Combines events in the search results that have a single differing field value into one result with a multi-value field of the differing field.",
      "syntax": "mvcombine (delim=<string>)? <field>",
      "tags": "combine merge join unite multivalue",
      "usage": "public"
   },
   "mvexpand": {
      "category": "results::generate",
      "comment1": "Create new events for each value of multi-value field, \"foo\".",
      "comment2": "Create new events for the first 100 values of multi-value field, \"foo\".",
      "description": "For each result with the specified field, create a new result for each value of that field in that result if it a multivalue field.",
      "example1": "... | mvexpand foo",
      "example2": "... | mvexpand foo limit=100",
      "related": "makemv, mvcombine, nomv",
      "shortdesc": "Expands the values of a multi-value field into separate events for each value of the multi-value field.",
      "syntax": "mvexpand <field> (limit=<int>)?",
      "tags": "separate divide disconnect multivalue",
      "usage": "public"
   },
   "nomv": {
      "category": "fields::convert",
      "comment": "For sendmail events, combine the values of the senders field into a single value; then, display the top 10 values.",
      "description": "Converts values of the specified multi-valued field into one single value (overrides multi-value field configurations set in fields.conf).",
      "example": "eventtype=\"sendmail\" | nomv senders | top senders",
      "related": "makemv, mvcombine, mvexpand, convert",
      "shortdesc": "Changes a specified multi-value field into a single-value field at search time.",
      "syntax": "nomv <field>",
      "tags": "single multivalue",
      "usage": "public"
   },
   "outlier": {
      "alias": "outlierfilter",
      "category": "reporting",
      "comment1": "Remove all outlying numerical values.",
      "comment2": "For a timechart of webserver events, transform the outlying average CPU values.",
      "description": "Removes or truncates outlying numerical values in selected fields. If no fields are specified, then outlier will attempt to process all fields.",
      "example1": "... | outlier",
      "example2": "404 host=\"webserver\" | timechart avg(cpu_seconds) by host | outlier action=tf",
      "related": "anomalies, anomalousvalue, cluster, kmeans",
      "shortdesc": "Removes outlying numerical values.",
      "syntax": "outlier (<outlier-option> )* (<field-list>)?",
      "tags": "outlier anomaly unusual odd irregular dangerous unexpected",
      "usage": "public"
   },
   "outputcsv": {
      "category": "results::write",
      "comment1": "Output search results to the CSV file 'mysearch.csv'.",
      "description": "If no filename specified, rewrites the contents of each result as a CSV row into the \"_xml\" field.\n             Otherwise writes into file (appends \".csv\" to filename if filename has no existing extension).\n             If singlefile is set to true and output spans multiple files, collapses it into a single file.\n             The option usexml=[t|f] specifies whether or not to encode the csv output into xml and has effect\n             only when no filename is specified.  This option should not specified when invoking outputcsv from\n             the UI.  If dispatch option is set to true, filename refers to a file in the job directory in\n             $SPLUNK_HOME/var/run/splunk/dispatch/<job id>/\n             If 'create_empty' is true and no results are passed to outputcsv, an 0-length file is created.\n             When false (the default) no file is created and the file is deleted if it previously existed.\n             If 'override_if_empty' is set to its default of true and no results are passed to outputcsv, the\n             command deletes the output file if it exists.  If set to false, the command does not delete the\n             existing output file.\n             If 'append' is true, we will attempt to append to an existing csv file if it exists or create a\n             file if necessary.  If there is an existing file that has a csv header already, we will only emit\n             the fields that are referenced by that header.  (Defaults to false)  .gz files cannot be append to.",
      "example1": "... | outputcsv mysearch",
      "related": "inputcsv",
      "shortdesc": "Outputs search results to the specified CSV file.",
      "syntax": "outputcsv (append=<bool>)? (create_empty=<bool>)? (override_if_empty=<bool>?) (dispatch=<bool>)? (usexml=<bool>)? (singlefile=<bool>)? (<filename>)?",
      "tags": "output csv save write",
      "usage": "public"
   },
   "outputlookup": {
      "category": "results::write",
      "comment1": "Write to \"users.csv\" lookup file (under $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/*/lookups).",
      "comment2": "Write to \"usertogroup\" lookup table (as specified in transforms.conf).",
      "description": "Saves results to a lookup table as specified by a filename (must end with .csv or .gz) or a table name (as specified by a stanza name in transforms.conf).  If the lookup file does not exist, we will by default create the file in the lookups directory of the current application.  If the 'createinapp' option is set to false or if there is no current application context, then we will create the file in the system lookups directory.\n             If 'create_empty' is true (the default) and no results are passed to outputlookup, an 0-length file is created.\n             When false no file is created and the file is deleted if it previously existed.\n             If 'override_if_empty' is set to its default of true and no results are passed to outputlookup, the\n             command deletes the lookup file if it exists.  If set to false, the command does not delete the\n             existing lookup file.\n             If 'key_field' is set to a valid field name and this is a key-value store-based lookup, we will\n             attempt to use the specified field as the key to a value and replace that value.\n             If 'append' is true, we will attempt to append to an existing csv file if it exists or create a\n             file if necessary.  If there is an existing file that has a csv header already, we will only emit\n             the fields that are referenced by that header.  (Defaults to false).  .gz files cannot be appended to.\n             The 'output_format' controls the output data format, supported values are 'splunk_mv_csv' and 'splunk_sv_csv'.\n             The default value is 'splunk_sv_csv'. Use 'splunk_mv_csv' for multivalue fields.",
      "example1": "| outputlookup users.csv",
      "example2": "| outputlookup usertogroup",
      "optout-in": "lite, lite_free",
      "related": "inputlookup, lookup, outputcsv, outputlookup",
      "shortdesc": "Saves search results to the specified static lookup table.",
      "syntax": "outputlookup (append=<bool>)? (create_empty=<bool>)? (override_if_empty=<bool>? (max=<int>)? (key_field=<field_name>)? (createinapp=<bool>)? (output_format=<string>) ? (<filename>|<string:tablename>)",
      "tags": "output csv save write lookup table",
      "usage": "public"
   },
   "outputtelemetry": {
      "category": "results::write",
      "comment1": "Output search results to the telemetry endpoint, using the field named \"data.\"\" Each will be\n\t\tnamed \"my.telemetry\" and is described as a singular \"event\" type. The telemetry event will only be\n\t\tsent if the deployment has been opted in to share Anonymized usage data, with opt-in version of 2.",
      "description": "Outputs search results to telemetry endpoint.\n             Required field input will have the endpoint payload.\n             The other fields component, type, optinrequired\n             are optional fields butthe endpoint expects them to be supplied either with the search command\n             or to be found in the event data.\n             Visibility fields \"anonymous\", \"license\" and \"support\" are optional.",
      "example1": "... fields data | outputtelemetry input=data component=my.telemetry type=event anonymous=true optinrequired=2",
      "shortdesc": "Outputs search results to telemetry endpoint.",
      "syntax": "outputtelemetry (input=<string>) (type=<string>)? (component=<string>)? (support=<bool>)? (anonymous=<bool>)? (license=<bool>)? (optinrequired=<int>)?",
      "tags": "output telemetry",
      "usage": "public"
   },
   "outputtext": {
      "category": "formatting",
      "comment1": "Output the \"_raw\" field of your current search into \"_xml\".",
      "description": "Rewrites the _raw field of the result into the \"_xml\" field.\n             If usexml is set to true (the default), the _raw field is\n             XML escaped.",
      "example1": "... | outputtext",
      "related": "outputcsv, outputraw",
      "shortdesc": "Outputs the raw text (_raw) of results into the _xml field.",
      "syntax": "outputtext (usexml=<bool>)?",
      "tags": "output",
      "usage": "public beta"
   },
   "overlap": {
      "category": "index::summary",
      "comment": "Find overlapping events in \"summary\".",
      "description": "Find events in a summary index that overlap in time, or\nfind gaps in time during which a scheduled saved search may have\nmissed events.  Note: If you find a gap, run the search over the period\nof the gap and summary index the results (using | collect). If you\nfind overlapping events, manually delete the overlaps from the summary\nindex by using the search language.  Invokes an external python script\n(in etc/searchscripts/sumindexoverlap.py), which expects input events\nfrom the summary index and finds any time overlaps and gaps between\nevents with the same 'info_search_name' but different\n'info_search_id'.  Input events are expected to have the following\nfields: 'info_min_time', 'info_max_time' (inclusive and exclusive,\nrespectively) , 'info_search_id' and 'info_search_name' fields.",
      "example": "index=summary | overlap",
      "optout-in": "lite, lite_free",
      "related": "collect sistats sitop sirare sichart sitimechart",
      "shortdesc": "Finds events in a summary index that overlap in time or have missed events.",
      "syntax": "overlap",
      "tags": "collect overlap index summary summaryindex",
      "usage": "public"
   },
   "pivot": {
      "category": "reporting",
      "description": "Must be the first command in a search. You must specify the model, object,\n             and the pivot element to run. The command will expand and run the specified\n             pivot element.",
      "related": "datamodel",
      "shortdesc": "Allows user to run pivot searches against a particular datamodel object.",
      "syntax": "pivot <datamodel-name> <object-name> <pivot-element>",
      "tags": "datamodel model pivot",
      "usage": "public"
   },
   "predict": {
      "category": "reporting",
      "comment1": "Predict foo using the default LLP5 algorithm (an algorithm that combines the LLP and LLT algorithms).",
      "comment2": "Upper and lower confidence intervals do not have to match",
      "comment3": "Illustrates the LLB algorithm. The foo2 field is predicted by correlating it with the foo1 field.",
      "comment4": "Predict multiple fields using the same algorithm. The default algorithm LLP5 is used in this example.",
      "comment5": "Predict multiple fields using the same algorithm, future_timespan, and holdback.",
      "comment6": "Use aliases for the fields by specifying the AS keyword for each field.",
      "comment7": "Predict multiple fields using different algorithms and different options for each field.",
      "comment8": "Predict foo1 and foo2 together using the bivariate algorithm BiLL.",
      "description": "The predict command must be preceded by the timechart command.\n The command can also fill in missing data in a time-series and\n provide predictions for the next several time steps. \\p\\\n The predict command provides confidence intervals for all of its estimates.\n The command adds a predicted value and an upper and lower 95th (by default)\n percentile range to each event in the time-series.",
      "example1": "... | timechart span=\"1m\" count AS foo1 | predict foo1",
      "example2": "... | timechart span=\"1m\" count AS foo | predict foo AS foobar algorithm=LL upper90=high lower97=low future_timespan=10 holdback=20",
      "example3": "... | timechart span=\"1m\" count(x) AS foo1 count(y) AS foo2 | predict foo2 AS foobar algorithm=LLB correlate=foo1 holdback=100",
      "example4": "... | timechart span=\"1m\" count AS foo1 avg(sales) AS foo2 sum(sales) AS foo3 |  predict foo1 foo2 foo3",
      "example5": "... timechart span=\"1m\" count AS foo1 avg(sales) AS foo2 sum(sales) AS foo3 | predict foo1 foo2 foo3 algorithm=LLT future_timespan=15 holdback=5",
      "example6": "... timechart span=\"1m\" count AS foo1 avg(sales) AS foo2 sum(sales) AS foo3 | predict foo1 AS foobar1 foo2 AS foobar2 foo3 AS foobar3 algorithm=LLT future_timespan=15 holdback=5",
      "example7": "... timechart span=\"1m\" count AS foo1 avg(sales) AS foo2 sum(sales) | predict foo1 algorithm=LL future_timespan=15 foo2 algorithm=LLP period=7 future_timespan=7",
      "example8": "... timechart span=\"1m\" count AS foo1 avg(sales) AS foo2 sum(sales) | predict foo1 foo2 algorithm=BiLL future_timespan=10",
      "related": "trendline, x11",
      "shortdesc": "Forecasts future values for one or more sets of time-series data.",
      "syntax": "predict <field-list>  <pd-as-option>? <pd-algo-option>? <pd-correlate-option>? <pd-future_timespan-option>? <pd-holdback-option>? <pd-period-option>? <pd-upper-option>? <pd-lower-option>? <pd-suppress-option>?",
      "tags": "forecast predict univariate bivariate kalman",
      "usage": "public"
   },
   "rangemap": {
      "category": "fields::add",
      "comment1": "Set RANGE to \"green\" if the date_second is between 1-30; \"blue\", if between 31-39; \"red\", if between 40-59; and \"gray\", if no range matches (e.g. \"0\").",
      "comment2": "Sets the value of each event's RANGE field to \"low\" if COUNT is 0, \"elevated\" if between 1-100, and \"severe\" otherwise.",
      "description": "Sets RANGE field to the names of any ATTRN that the value of FIELD is within.  If no range is matched, the RANGE is set to the DEFAULT values.",
      "example1": "... | rangemap field=date_second green=1-30 blue=31-39 red=40-59 default=gray",
      "example2": "... | rangemap field=count low=0-0 elevated=1-100 default=severe",
      "shortdesc": "Sets RANGE field to the name of the ranges that match.",
      "syntax": "rangemap field=<field> (<attrn>=<attrn-range>)+ (default=<string>)?",
      "tags": "colors stoplight range",
      "usage": "public"
   },
   "rare": {
      "category": "reporting",
      "comment1": "Find the least common \"user\" value for a \"host\".",
      "commentcheat": "Return the least common values of the \"url\" field.",
      "description": "Finds the least frequent tuple of values of all fields in the field list.\n           If optional by-clause is specified, this command will return rare tuples of values for\n           each distinct tuple of values of the group-by fields.",
      "example1": "... | rare user by host",
      "examplecheat": "... | rare url",
      "related": "top, stats, sirare",
      "shortdesc": "Displays the least common values of a field.",
      "supports-multivalue": "true",
      "syntax": "rare <rare-command-arguments>",
      "tags": "rare few occasional scarce sparse uncommon unusual",
      "usage": "public"
   },
   "redistribute": {
      "category": "data::managing",
      "comment1": "Speeds up a stats search that aggregates a large number of results.\n         The \"|stats count by ip\" portion of the search is processed on the\n         intermediate reducers. The search head just aggregates the results.",
      "comment2": "Speeds up a search that includes eventstats and which uses\n         sitimechart to perform the statistical calculations for a timechart\n         operation. The intermediate reducers process eventstats, where, and\n         sitimechart. The search head runs timechart to turn the reduced\n         sitimechart statistics into sorted, visualization-ready results.\n         Because the redistribute split-by field is unidentified, the system\n         selects \"source\" as the redistribute field.",
      "comment3": "Speeds up a search that uses tstats to generate events. The\n         tstats command must be placed at the start of the search pipeline,\n         and here it uses prestats=t to work with the timechart command.\n         sitimechart is processed on the reducers and timechart is processed on\n         the search head.",
      "comment4": "In this example, the eventstats and where commands are processed\n         in parallel on the reducers, while the sort command and any commands\n         following it are processed on the search head. This happens because\n         sort is a nonstreaming command that is not supported by redistribute.",
      "description": "This command divides the search results\n         among a pool of intermediate reducers in the indexer layer. The reducers\n         perform intermediary reduce operations in parallel on the search results\n         before pushing them up to the search head, where a final reduction\n         operation is performed. This parallelization of reduction work that\n         would otherwise be done entirely by the search head can result in\n         faster completion times for high-cardinality searches that\n         aggregate large numbers of search results. \\p\\\n         Set num_of_reducers to control the number of intermediate reducers\n         used from the pool. num_of_reducers defaults to a fraction of the\n         indexer pool size, according to the 'winningRate' setting, and is\n         limited by the 'maxReducersPerPhase' setting, both of which are\n         specified on the search head in the [parallelreduce] stanza of\n         limits.conf. \\p\\\n         The redistribute command divides events into partitions on the\n         intermediate reducers according to the fields specified with the\n         by-clause. If no by-clause fields are specified, the search\n         processor uses the fields that work best with the commands that\n         follow the redistribute command in the search. \\p\\\n         The redistribute command requires a distributed search environment\n         with a pool of intermediate reducers at the indexer level. You must\n         have a role with the run_multi_phased_searches capability to run\n         this command. You can use the redistribute command only once in a\n         search. \\p\\\n         The redistribute command supports streaming commands and the\n         following nonstreaming commands: stats, tstats, streamstats,\n         eventstats, sichart, and sitimechart. The redistribute command also\n         supports transaction on a single field. \\p\\\n         The redistribute command moves the processing of a search string\n         from the intermediate reducers to the search head when it\n         encounters nonstreaming command that it does not support or that\n         does not include a by-clause. The redistribute command also moves\n         processing to the search head when it detects that a command has\n         modified values of the fields specified in the redistribute by-clause. \\p\\\n         Note: When results are aggregated from the intermediate reducers at\n         the search head, a sort order is imposed on the result rows only\n         when an order-sensitive command such as 'sort' is in place to\n         consume the reducer output.",
      "example1": "\"... | redistribute by ip | stats count by ip\"",
      "example2": "\"... | redistribute | eventstats count by user, source | where  count>10 | sitimechart max(count) by source | timechart max(count) by source\"",
      "example3": "\"| tstats prestats=t count BY _time span=1d | redistribute by _time | sitimechart span=1d count | timechart span=1d count\"",
      "example4": "\"... | redistribute | eventstats count by user, source | where count >10  | sort 0 -num(count) | ...\"",
      "shortdesc": "Speeds up search runtime of a set of supported SPL commands, in a\n         distributed search environment.",
      "supports-multivalue": "true",
      "syntax": "redistribute (num_of_reducers=<int>)? (<by-clause>)?",
      "tags": "partition re-partition repartition shuffle collocate",
      "usage": "public"
   },
   "regex": {
      "category": "results::filter",
      "commentcheat": "Keep only search results whose \"_raw\" field contains IP addresses in the non-routable class A (10.0.0.0/8).",
      "description": "Removes results that do not match the specified regular expression. You can specify for the regex to keep results that match the expression, or to keep those that do not match.  Note: if you want to use the \"or\" (\"|\") command in a regex argument, the whole regex expression must be surrounded by quotes (ie. regex \"expression\"). Matches the value of the field against the unanchored regex and only keeps those events that match in the case of '=' or do not match in the case of '!='. If no field is specified, the match is against \"_raw\".",
      "example1": "... | regex _raw=\"complicated|regex(?=expression)\"",
      "example2": "... | regex _raw=\"(?=!\\d)10.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(?!\\d)\"",
      "examplecheat": "... | regex _raw=\"(?<!\\d)10.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(?!\\d)\"",
      "related": "rex, search",
      "shortdesc": "Removes results that do not match the specified regular expression.",
      "syntax": "regex (<field>(\"=\"|\"!=\"))?<regex-expression>",
      "tags": "regex regular expression filter where",
      "usage": "public"
   },
   "relevancy": {
      "category": "fields::add",
      "comment1": "Calculate the relevancy of the search and sort the results in descending order.",
      "description": "Calculates the 'relevancy' field based on how well the events _raw field matches the keywords of the 'search'.  Useful for retrieving the best matching events/documents, rather than the default time-based ordering. Events score a higher relevancy if they have more rare search keywords, more frequently, in fewer terms.  For example a search for \"disk error\" will favor a short event/document that has 'disk' (a rare term) several times and 'error' once, than a very large event that has 'disk' once and 'error' several times.",
      "example1": "disk error | relevancy | sort -relevancy",
      "related": "abstract, highlight, sort",
      "shortdesc": "Calculates how well the event matches the query.",
      "syntax": "relevancy",
      "tags": "search relevance precision text doc ir",
      "usage": "public"
   },
   "reltime": {
      "category": "formatting",
      "comment1": "add a reltime field",
      "description": "Sets the 'reltime' field to a human readable value of the difference between 'now' and '_time'.  Human-readable values look like \"5 days ago\", \"1 minute ago\", \"2 years ago\", etc.",
      "example1": "... | reltime",
      "related": "convert",
      "shortdesc": "Sets the 'reltime' field to a human readable value of the difference between 'now' and '_time'.",
      "syntax": "reltime",
      "tags": "time ago",
      "usage": "public beta"
   },
   "rename": {
      "category": "fields::modify",
      "comment1": "Rename the \"count\" field.",
      "comment2": "Rename fields beginning with \"foo\".",
      "commentcheat": "Rename the \"_ip\" field as \"IPAddress\".",
      "description": "Renames a field. If both the source and destination fields are\n             wildcard expressions with he same number of wildcards,\n             the renaming will carry over the wildcarded portions to the\n             destination expression.",
      "example1": "... | rename count as \"Count of Events\"",
      "example2": "... | rename foo* as bar*",
      "examplecheat": "... | rename _ip as IPAddress",
      "related": "fields",
      "shortdesc": "Renames a specified field (wildcards can be used to specify multiple fields).",
      "syntax": "rename (<wc-field> as <wc-field>)+",
      "tags": "rename alias name as aka",
      "usage": "public"
   },
   "replace": {
      "category": "fields::modify",
      "commentcheat": "Change any host value that ends with \"localhost\" to \"localhost\".",
      "description": "Replaces a single occurrence of the first string with the second\n             within the specified fields (or all fields if none were specified).\n             Non-wildcard replacements specified later take precedence over those specified earlier.\n             For wildcard replacement, fuller matches take precedence over lesser matches.\n             To assure precedence relationships, one is advised to split the replace into\n             two separate invocations.\n             When using wildcarded replacements, the result must have the same number\n             of wildcards, or none at all.\n             Wildcards (*) can be used to specify many values to replace, or replace values with.",
      "example1": "... | replace 127.0.0.1 with localhost",
      "example2": "... | replace 127.0.0.1 with localhost in host",
      "example3": "... | replace 0 with Critical, 1 with Error in msg_level",
      "example4": "... | replace aug with August in start_month end_month",
      "example5": "... | replace *localhost with localhost in host",
      "example6": "... | replace \"* localhost\" with \"localhost *\" in host",
      "examplecheat": "... | replace *localhost with localhost in host",
      "related": "fillnull setfields rename",
      "shortdesc": "Replaces values of specified fields with a specified new value.",
      "syntax": "replace (<wc-str> with <wc-str>)+ (in <field-list>)?",
      "tags": "replace change set",
      "usage": "public"
   },
   "rest": {
      "category": "rest",
      "comment1": "Access saved search jobs.",
      "description": "Access a REST endpoint and display the returned entities as search results.",
      "example1": "| rest /services/search/jobs count=0 splunk_server=local | search isSaved=1",
      "generating": "yes",
      "syntax": "rest <rest-uri> (count=<int>)? (<splunk-server-opt>)? (<splunk-server-group-opt>)* (<timeout-opt>)? (<get-arg-name>=<get-arg-value>)*",
      "tags": "rest endpoint access",
      "usage": "public"
   },
   "return": {
      "category": "search::subsearch",
      "comment1": "search for \"error ip=<someip>\", where someip is the most recent ip used by Amrit",
      "comment2": "search for \"error (user=user1 ip=ip1) OR (user=user2 ip=ip2)\", where users and IPs come from the two most-recent logins",
      "description": "Useful for passing values up from a subsearch.  Replaces the incoming events with one event, with one attribute: \"search\". Automatically limits the incoming results with \"head\" and \"fields\", to improve performance.  Allows convenient outputting of attr=value (e.g., \"return source\"), alias_attr=value (e.g. \"return ip=srcip\"), and value (e.g., \"return $srcip\").  Defaults to using just the first row of results handed to it.  Multiple rows can be specified with COUNT (e.g. \"return 2 ip\"), and each row is ORd (e.g., output might be \"(ip=10.1.11.2) OR (ip=10.2.12.3)\").  Multiple values can be specified and are placed within OR clauses.  So \"return 2 user ip\" might output \"(user=bob ip=10.1.11.2) OR (user=fred ip=10.2.12.3)\".  Using \"return\" at the end of a subsearch removes the need, in the vast majority of cases, for \"head\", \"fields\", \"rename\", \"format\", and \"dedup\".",
      "example1": "error [ search user=amrit | return ip]",
      "example2": "error [ search login | return 2 user, ip]",
      "related": "search format",
      "shortdesc": "A convenient way to return values from a subsearch.",
      "syntax": "return (<int:count>)? (<field:alias>=<field>)* (<field>)* ($<field>)*",
      "tags": "format query subsearch search",
      "usage": "public"
   },
   "reverse": {
      "category": "results::order",
      "commentcheat": "Reverse the order of a result set.",
      "description": "Reverses the order of the results.",
      "examplecheat": "... | reverse",
      "related": "head, sort, tail",
      "syntax": "reverse",
      "tags": "reverse flip invert inverse upsidedown",
      "usage": "public"
   },
   "rex": {
      "category": "fields::add",
      "comment1": "Anonymize data matching pattern",
      "commentcheat": "Extract \"from\" and \"to\" fields using regular expressions. If a raw event contains \"From: Susan To: Bob\", then from=Susan and to=Bob.",
      "description": "Matches the value of the field against the unanchored regex and extracts\n             the perl regex named groups into fields of the corresponding names. If\n             mode is set to 'sed' the given sed expression will be applied to the value\n             of the chosen field (or to _raw if a field is not specified).\n             max_match controls the number of times the regex is matched, if greater than one\n             the resulting fields will be multivalued fields, defaults to 1, use 0 to mean unlimited.",
      "example1": "... | rex mode=sed \"s/(\\\\d{4}-){3}/XXXX-XXXX-XXXX-/g\"",
      "examplecheat": "... | rex field=_raw \"From: (?<from>.*) To: (?<to>.*)\"",
      "related": "extract, kvform, multikv, xmlkv, regex",
      "shortdesc": "Specifies a Perl regular expression named groups to extract fields while you search.",
      "syntax": "rex (field=<field>)? ( ( <regex-expression> (max_match=<int>)? (offset_field=<string>)? ) | mode=sed <sed-expression>)",
      "tags": "regex regular expression extract",
      "usage": "public"
   },
   "rtorder": {
      "comment1": "Keep a buffer of the last 5 minutes of events, emitting events in ascending time order after\n          the events are more than 5 minutes old.  Newly received events that are older than 5 minutes\n          are discarded if an event after that time has already been emitted.",
      "description": "The rtorder command creates a streaming event buffer that takes input events, stores them\n             in the buffer in ascending time order. The events are emitted in that order from the\n             buffer only after the current time reaches at least the span of time given by buffer_span\n             after the timestamp of the event.  The buffer_span is by default 10 seconds.\n             Events are emitted from the buffer if the maximum size of the buffer is exceeded.\n             The default max_buffer_size is 50000, or the max_result_rows setting of the [search]\n             stanza in the limits.conf file.  If an event is received as input that is earlier\n            than an event that has been emitted previously, that out of order event is emitted\n             immediately unless the discard option is set to true (it is false by default).\n             When discard is set to true, out of order events are discarded, assuring that the\n            output is always strictly in time ascending order.",
      "example1": "... | rtorder discard=t buffer_span=5m",
      "related": "sort",
      "shortdesc": "Buffers events from real-time search to emit them in ascending time order when possible.",
      "syntax": "rtorder (discard=<bool>)? (buffer_span=<span-length>)? (max_buffer_size=<int>)?",
      "tags": "realtime sort order",
      "usage": "public"
   },
   "savedsearch": {
      "alias": "macro, savedsplunk",
      "category": "results::generate",
      "comment1": "Run the searchindex command with an index provided (as per above)",
      "commentcheat": "Run the \"mysecurityquery\" saved search.",
      "description": "Runs a saved search.\n             If the search contains replacement terms, will perform string replacement.\n             For example, if the search were something like \"index=$indexname$\", then\n             the indexname term can be provided at invocation time of the savedsearch command.",
      "example1": "| savedsearch searchindex index=main",
      "examplecheat": "| savedsearch mysecurityquery",
      "related": "search",
      "shortdesc": "Runs a saved search by name.",
      "syntax": "savedsearch <string> (<savedsearch-opt> )*",
      "tags": "search macro saved bookmark",
      "usage": "public"
   },
   "script": {
      "alias": "run",
      "category": "search::external",
      "comment1": "Run the Python script \"myscript\" with arguments, myarg1 and myarg2; then, email the results.",
      "description": "Calls an external python program that can modify or generate search results.\n             Scripts must be declared in commands.conf and be located in \"$SPLUNK_HOME/etc/apps/app_name/bin\".\n             The  scripts are run with \"$SPLUNK_HOME/bin/python\".",
      "example1": "... | script python myscript myarg1 myarg2 | sendemail to=david@splunk.com",
      "optout-in": "lite, lite_free",
      "shortdesc": "Runs an external Python-implemented search command.",
      "syntax": "script <script-name-arg> (<script-arg> )* (<maxinputs-opt>)?",
      "tags": "script run python perl custom",
      "usage": "public"
   },
   "scrub": {
      "category": "formatting",
      "comment1": "Anonymize the current search results.",
      "description": "Anonymizes the search results by replacing identifying data - usernames, IP addresses, domain names, etc. - with fictional values that maintain the same word length. For example, it may turn the string user=carol@adalberto.com into user=aname@mycompany.com. This lets Splunk users share log data without revealing confidential or personal information. By default the dictionary and configuration files found in $SPLUNK_HOME/etc/anonymizer are used.  These can be overridden by specifying arguments to the scrub command.  The arguments exactly correspond to the settings in the stand-alone \"splunk anonymize\" command, and are documented there.  Anonymizes all attributes, exception those that start with \"_\" (except \"_raw\") or \"date_\", or the following attributes: \"eventtype\", \"linecount\", \"punct\", \"sourcetype\", \"timeendpos\", \"timestartpos\".",
      "When using alternative filenames, they must not contain paths and refer to files located in $SPLUNK_HOME/etc/anonymizer, or the optional namespace": "\"appname\" must be used to specify an app supplying the files, and they will be read from $SPLUNK_HOME/etc/app/<appname>/anonymizer.",
      "example1": "... | scrub",
      "shortdesc": "Anonymizes the search results.",
      "syntax": "scrub (public-terms=<filename>)? (private-terms=<filename>)? (name-terms=<filename>)? (dictionary=<filename>)? (timeconfig=<filename>)? (namespace=<string>)?",
      "tags": "anonymize scrub secure private obfuscate",
      "usage": "public beta"
   },
   "search": {
      "category": "search::search",
      "comment1": "Search for events with \"404\" and from host \"webserver1\"",
      "comment2": "Search for events with either codes 10 or 29, and a host that isn't \"localhost\" and an xqp that is greater than 5",
      "commentcheat1": "Keep only search results that have the specified \"src\" or \"dst\" values.",
      "description": "If the first search command, retrieve events from the indexes, using keywords, quoted phrases, wildcards, and key/value expressions; if not the first, filter results.",
      "example1": "404 host=\"webserver1\"",
      "example2": "(code=10 OR code=29) host!=\"localhost\" xqp>5",
      "examplecheat1": "src=\"10.9.165.*\" OR dst=\"10.9.165.8\"",
      "shortdesc": "Filters results using keywords, quoted phrases, wildcards, and key/value expressions.",
      "simplesyntax": "search <logical-expression>?",
      "syntax": "search <logical-expression>?",
      "tags": "search query find where filter daysago enddaysago endhoursago endminutesago endmonthsago endtime endtime eventtype eventtypetag host hosttag hoursago minutesago monthsago searchtimespandays searchtimespanhours searchtimespanminutes searchtimespanmonths source sourcetype startdaysago starthoursago startminutesago startmonthsago starttime starttimeu tag",
      "usage": "public"
   },
   "searchtxn": {
      "category": "results::group",
      "comment1": "find all email transactions to root from david smith",
      "description": "Retrieves events matching the transactiontype\nTRANSACTION-NAME with events transitively discovered by the initial\nevent constraint of the SEARCH-STRING.  \\p\\\nFor example, given an 'email'",
      "transactiontype with fields": "\"qid pid\" and with a search attribute of",
      "'sourcetype": "\"sendmail_syslog\" ((qid=val1 pid=val1) OR ...",
      "find all the events that match 'sourcetype": "\"sendmail_syslog\" to=root'.\\p\\\nFrom those results, all the qid's and pid's are transitively used to\nfind further search for relevant events. When no more qid or pid\nvalues are found, the resulting search is run\\i\\",
      "....(qid": "valn pid=valm) | transaction name=email | search to=root'.\\p\\\nOptions:\\p\\",
      "max_terms -- integer between 1-1000 which determines how many unique field values all fields can use (default": "1000).  Using smaller values will speed up search, favoring more recent values\\p\\",
      "use_disjunct -- determines if each term in SEARCH-STRING should be OR'd on the initial search (default": "true)\\p\\",
      "eventsonly -- if true, only the relevant events are retrieved, but the \"|transaction\" command is not run (default": "false)",
      "example1": "| searchtxn email to=root from=\"david smith\"",
      "related": "transaction",
      "shortdesc": "Finds transaction events given search constraints.",
      "syntax": "searchtxn <transaction-name> (max_terms=<int>)? (use_disjunct=<bool>)? (eventsonly=<bool>)? <search-string>",
      "tags": "transaction group cluster collect gather needle winnow",
      "usage": "public"
   },
   "selfjoin": {
      "category": "results::filter",
      "comment1": "Join results with itself on 'id' field.",
      "description": "Join results with itself, based on a specified field or list of fields to join on.",
      "example1": "... | selfjoin id",
      "related": "join",
      "shortdesc": "Joins results with itself.",
      "syntax": "selfjoin (<selfjoin-options>)* <field-list>",
      "tags": "join combine unite",
      "usage": "public"
   },
   "sendemail": {
      "category": "alerting",
      "comment1": "Send search results to the specified email.",
      "comment2": "Send search results in HTML format with the subject \"myresults\".",
      "description": "Emails search results to the specified email addresses.",
      "example1": "... | sendemail to=\"elvis@splunk.com\"",
      "example2": "... | sendemail to=\"elvis@splunk.com,john@splunk.com\" content_type=html subject=myresults server=mail.splunk.com",
      "shortdesc": "Emails search results to the specified email addresses.",
      "syntax": "sendemail <to-option> <from-option>? <cc-option>? <bcc-option>? <subject-option>? <message-option>? <footer-option>? <sendresults-option>? <inline-option>? <format-option>? <sendcsv-option>? <sendpdf-option>? <pdfview-option>? (<paperorientation-option>)? <papersize-option>? <priority-option>? <server-option>? <graceful-option>? <content_type-option>? <width_sort_columns-option>? <use_ssl-option>? <use_tls-option>? <maxinputs-option>? <maxtime-option>?",
      "tags": "email mail alert",
      "usage": "public"
   },
   "set": {
      "category": "search::subsearch",
      "comment1": "Return all urls that have 404 errors and 303 errors.",
      "commentcheat": "Return values of \"URL\" that contain the string \"404\" or \"303\" but not both.",
      "description": "Performs two subsearches and then executes the specified set operation on the two sets of search results.",
      "example1": "| set intersect [search 404 | fields url] [search 303 | fields url]",
      "examplecheat": "| set diff [search 404 | fields url] [search 303 | fields url]",
      "generating": "true",
      "related": "append, appendcols, join, diff",
      "shortdesc": "Performs set operations on subsearches.",
      "syntax": "set (union|diff|intersect) <subsearch> <subsearch>",
      "tags": "diff union join intersect append",
      "usage": "public"
   },
   "shape": {
      "category": "reporting",
      "comment1": "group each dest_ip into a separate transaction, keeping a list of all their delay values, and then calculating the shape of that dest_ip's delays. dest_ips can be subsequently be clustered by the shape of their delays.",
      "description": "Given a numeric multivalued FIELD, produce a 'shape'\nattribute, describing the shape of the values, in a symbolic\nrepresentation.  The symbolic representation will be at most MAXVALUES\nlong and have at most MAXRESOLUTION different characters.  The\ndefaults are MAXVALUES = 5 and MAXRESOLUTION = 10, normally producing\na SHAPE value of 5 characters made up of 10 letters (a-k).",
      "example1": "... | fields dest_ip, delay | transaction dest_ip mvlist=true | shape delay",
      "related": "anomalousvalue, cluster, kmeans, outlier",
      "shortdesc": "Produces a symbolic 'shape' attribute describing the shape of a numeric multivalued field.",
      "syntax": "shape <field> (maxvalues=<int>)? (maxresolution=<int>)?",
      "tags": "summary symbolic",
      "usage": "public"
   },
   "sichart": {
      "category": "index::summary",
      "comment1": "Compute the necessary information to later do 'chart avg(foo) by bar' on summary indexed results.",
      "description": "Summary indexing friendly versions of chart command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.",
      "example1": "... | sichart avg(foo) by bar",
      "optout-in": "lite, lite_free",
      "related": "collect, overlap, sirare, sistats, sitimechart, sitop",
      "shortdesc": "Summary indexing friendly versions of chart command.",
      "syntax": "sichart <chart-command-arguments>",
      "tags": "chart summary index summaryindex",
      "usage": "public"
   },
   "sirare": {
      "category": "index::summary",
      "comment1": "Compute the necessary information to later do 'rare foo bar' on summary indexed results.",
      "description": "Summary indexing friendly versions of rare command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.",
      "example1": "... | sirare foo bar",
      "optout-in": "lite, lite_free",
      "related": "collect, overlap, sichart, sistats, sitimechart, sitop",
      "shortdesc": "Summary indexing friendly versions of rare command.",
      "syntax": "sirare <rare-command-arguments>",
      "tags": "rare summary index summaryindex",
      "usage": "public"
   },
   "sistats": {
      "category": "index::summary",
      "comment1": "Compute the necessary information to later do 'stats avg(foo) by bar' on summary indexed results",
      "description": "Summary indexing friendly versions of stats command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.",
      "example1": "... | sistats avg(foo) by bar",
      "optout-in": "lite, lite_free",
      "related": "collect, overlap, sichart, sirare, sitop, sitimechart,",
      "shortdesc": "Summary indexing friendly versions of stats command.",
      "syntax": "sistats <stats-command-arguments>",
      "tags": "stats summary index summaryindex",
      "usage": "public"
   },
   "sitimechart": {
      "category": "index::summary",
      "comment1": "Compute the necessary information to later do 'timechart avg(foo) by bar' on summary indexed results.",
      "description": "Summary indexing friendly versions of timechart command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.",
      "example1": "... | sitimechart avg(foo) by bar",
      "optout-in": "lite, lite_free",
      "related": "collect, overlap, sichart, sirare, sistats, sitop",
      "shortdesc": "Summary indexing friendly versions of timechart command.",
      "syntax": "sitimechart <timechart-command-arguments>",
      "tags": "timechart summary index summaryindex",
      "usage": "public"
   },
   "sitop": {
      "category": "index::summary",
      "comment1": "Compute the necessary information to later do 'top foo bar' on summary indexed results.",
      "description": "Summary indexing friendly versions of top command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.",
      "example1": "... | sitop foo bar",
      "optout-in": "lite, lite_free",
      "related": "collect, overlap, sichart, sirare, sistats, sitimechart",
      "shortdesc": "Summary indexing friendly versions of top command.",
      "syntax": "sitop <top-command-arguments>",
      "tags": "top summary index summaryindex",
      "usage": "public"
   },
   "sort": {
      "category": "results::order",
      "comment1": "Sort results by the \"_time\" field in ascending order and then by the \"host\" value in descending order.",
      "comment2": "Sort first 100 results in descending order of the \"size\" field and then by the \"source\" value in ascending order.",
      "commentcheat": "Sort results by \"ip\" value in ascending order and then by \"url\" value in descending order.",
      "description": "Sorts by the given list of fields. If more than one field is specified,\n             the first denotes the primary sort order, the second denotes the secondary, etc.\n             If the fieldname is immediately (no space) preceded by \"+\", the sort is ascending (default).\n             If the fieldname is immediately (no space) preceded by \"-\", the sort is descending.\n             If white space follows \"+/-\", the sort order is applied to all following fields without a different explicit sort order.\n             Also a trailing \"d\" or \"desc\" causes the results to be reversed.\n             Results missing a given field are treated as having the smallest or largest\n             possible value of that field if the order es descending or ascending respectively.\n             If the field takes on numeric values, the collating sequence is numeric.\n             If the field takes on IP address values, the collating sequence is for IPs.\n             Otherwise, the collating sequence is lexicographic ordering.\n             If the first term is a number, then at most that many results are returned (in order).\n             If no number is specified, the default limit of 10000 is used.  If number is 0, all results will be returned.",
      "example1": "... | sort _time, -host",
      "example2": "... | sort 100 -size, +source",
      "examplecheat": "... | sort ip, -url",
      "related": "reverse",
      "shortdesc": "Sorts search results by the specified fields.",
      "simplesyntax": "sort (<int:count>)? <sort-by-clause>+ desc?",
      "syntax": "sort (<int>)? <sort-by-clause>+ (d|desc)?",
      "tags": "arrange, order, rank, sort",
      "usage": "public"
   },
   "spath": {
      "category": "fields::add",
      "description": "When called with no path argument, spath extracts all fields from the\n           first 5000 (limit is configurable via limits.conf characters, with the produced fields named by their path.\n           If a path is provided, the value of this path is extracted to a field\n           named by the path by default, or to a field specified by the output\n           argument if it is provided.\n           Paths are of the form 'foo.bar.baz'.  Each level can also have an\n           optional array index, delineated by curly brackets ex 'foo{1}.bar'.\n           All array elements can be represented by empty curly brackets e.g. 'foo{}'.\n           The final level for XML queries can also include an attribute name,\n           also enclosed by curly brackets,  e.g. 'foo.bar{@title}'.\n           By default, spath takes the whole event as its input.  The input\n           argument can be used to specify a different field for the input source.",
      "example1": "... | spath output=myfield path=foo.bar.baz",
      "example2": "... | spath input=oldfield output=newfield path=catalog.book{@id}",
      "example3": "... | spath server.name",
      "related": "rex, regex",
      "shortdesc": "Extracts values from structured data (XML or JSON) and stores them in a field or fields.",
      "syntax": "spath (output=<field>)? (path=<datapath> | <datapath>)? (input=<field>)?",
      "tags": "spath xpath json xml extract",
      "usage": "public"
   },
   "stats": {
      "category": "reporting",
      "commentcheat1": "Remove duplicates of results with the same \"host\" value and return the total count of the remaining results.",
      "commentcheat2": "Return the average for each hour, of any unique field that ends with the string \"lay\" (for example, delay, xdelay, relay, etc).",
      "commentcheat3": "Search the access logs, and return the number of hits from the top 100 values of \"referer_domain\".",
      "description": "Calculate aggregate statistics over the dataset, optionally grouped by a list of fields.\n             Aggregate statistics include: \\i\\\n                * count, distinct count \\i\\\n                * mean, median, mode \\i\\\n                * min, max, range, percentiles \\i\\\n                * standard deviation, variance \\i\\\n                * sum \\i\\\n                * earliest and latest occurrence \\i\\\n                * first and last (according to input order into stats command) occurrence \\p\\\n             Similar to SQL aggregation.\n             If called without a by-clause, one row is produced, which represents the\n             aggregation over the entire incoming result set. If called with a\n             by-clause, one row is produced for each distinct value of the by-clause.\n             The 'partitions' option, if specified, allows stats to partition the\n             input data based on the split-by fields for multithreaded reduce.\n             The 'allnum' option, if true (default = false), computes numerical statistics on each\n             field if and only if all of the values of that field are numerical.\n             The 'delim' option is used to specify how the values in the 'list' or 'values' aggregation are delimited.  (default is a single space)\n             When called with the name \"prestats\", it will produce intermediate results (internal).",
      "example1": "sourcetype=access* | stats avg(kbps) by host",
      "example2": "sourcetype=access* | top limit=100 referer_domain | stats sum(count)",
      "examplecheat1": "... | stats distinct_count(host)",
      "examplecheat2": "... | stats avg(*lay) BY date_hour",
      "examplecheat3": "sourcetype=access_combined | top limit=100 referer_domain | stats sum(count)",
      "note": "When called without any arguments, stats assumes the argument \"default(*)\".\n       This produces a table with the cross-product of aggregator and field as columns,\n       And a single row with the value of that aggregator applied to that field across all data.",
      "related": "eventstats, rare, sistats, streamstats, top",
      "shortdesc": "Provides statistics, grouped optionally by field.",
      "simplesyntax": "stats (((c|count|dc|distinct_count|estdc|estdc_error|earliest|latest|avg|stdev|stdevp|var|varp|sum|min|max|mode|median|first|last|earliest|latest|percint|list|values|range) \"(\" <field> \")\") (as <field>)? )+ (by <field-list>)?",
      "supports-multivalue": "true",
      "syntax": "stats <stats-command-arguments>",
      "tags": "stats statistics event sparkline count dc mean avg stdev var min max mode median",
      "usage": "public"
   },
   "strcat": {
      "category": "fields::add",
      "comment1": "Add a field, address, which combines the host and port values into the format <host>::<port>.",
      "comment2": "Add the field, comboIP, and then create a chart of the number of occurrences of the field values.",
      "commentcheat": "Add the field, comboIP, which combines the source and destination IP addresses and separates them with a front slash character.",
      "description": "Stitch together fields and/or strings to create a new field.\nQuoted tokens are assumed to be literals and the rest field names.\nThe destination field name is always at the end.\nIf allrequired=t, for each event the destination field is only\nwritten to if all source fields exist.  If allrequired=f (default)\nthe destination field is always written and any source fields\nthat do not exist are treated as empty string.",
      "example1": "... | strcat host \"::\" port address",
      "example2": "host=\"mailserver\" | strcat sourceIP \"/\" destIP comboIP | chart count by comboIP",
      "examplecheat": "... | strcat sourceIP \"/\" destIP comboIP",
      "related": "eval",
      "shortdesc": "Concatenates string values.",
      "syntax": "strcat (allrequired=<bool>)? <srcfields> <field>",
      "tags": "strcat concat string append",
      "usage": "public"
   },
   "streamstats": {
      "category": "reporting",
      "comment1": "For each event, add a count field that represent the number of events seen so far (including that event).  For example, 1 for the first event, 2 for the second, and so on.",
      "comment2": "For each event, compute the average of field foo over the last 5 events (including the current event).  Similar to doing trendline sma5(foo)",
      "comment3": "Same as example1, except that the current event is not included in the count",
      "comment4": "Compute the average value of foo for each value of bar including only the only 5 events with that value of bar.",
      "description": "Similar to the 'eventstats' command except that only events seen before\n       the current event (plus that event itself if current=t, which is the default)\n       are used to compute the aggregate statistics that are applied to each event. \\p\\\n       The 'window' option specifies the window size, based on number of events, to\n       use in computing the statistics. If set to 0, the default, all previous events\n       and the current event are used. If the 'global' option is set to false\n       (default is true) and 'window' is set to a non-zero value, a separate window\n       is used for each group of values of the group by fields. \\p\\\n       The 'allnum' option has the same affect as for the stats and eventstats\n       commands. \\p\\\n       If the reset_on_change option is set to true (default is false), all\n       accumulated information is reset (as if no previous events have been seen)\n       whenever the group by fields change. Events that do not have all of the\n       group by fields are ignored and will not cause a reset. \\p\\\n       The reset_before and reset_after arguments use boolean eval expressions.\n       When these expressions evaluated on a event either before or after\n       (respectively) the streamstats is calculation applied, will reset the\n       accumulated information. The reset_after condition might reference\n       fields emitted by the streamstats operation itself, whereas the reset_before\n       condition might not. When the reset options are combined with the 'window'\n       option, the window is also reset (to as if no previous events have been seen)\n       whenever the accumulated statistics are reset.\\p\\\n       If 'time_window' is specified, the window size limited by the range of _time\n       values in a window. A maximum number of events in a window still applies for\n       a time-based window. The default maximum is set in the max_stream_window\n       attribute in the limits.conf file. You can lower the maximum by specifying\n       the 'window' option.  The time_window option requires\n       the input events be sorted in either ascending or descending time order.",
      "example1": "... | streamstats count",
      "example2": "... | streamstats avg(foo) window=5",
      "example3": "... | streamstats count current=f",
      "example4": "... | streamstats avg(foo) by bar window=5 global=f",
      "related": "accum, autoregress, delta, eventstats, stats, streamstats, trendline",
      "shortdesc": "Adds summary statistics to all search results in a streaming manner.",
      "syntax": "streamstats (reset_on_change=<bool>)? (reset_before=\"(\"<eval-expression>\")\")? (reset_after=\"(\"<eval-expression>\")\")? (current=<bool>)? (window=<int>)? (time_window=<span-length>)? (global=<bool>)? (allnum=<bool>)? (<stats-agg-term>)* (<by-clause>)?",
      "tags": "stats statistics event",
      "usage": "public"
   },
   "table": {
      "category": "results::filter",
      "comment1": "Resulting table has field foo then bar then all fields that start with 'baz'",
      "description": "Returns a table formed by only the fields specified in the arguments. Columns are\n             displayed in the same order that fields are specified. Column headers are the field\n             names. Rows are the field values. Each row represents an event.",
      "example1": "... | table foo bar baz*",
      "related": "fields",
      "shortdesc": "Returns a table formed by only the fields specified in the arguments.",
      "syntax": "table <wc-field-list>",
      "tags": "fields",
      "usage": "public"
   },
   "tags": {
      "category": "fields::add",
      "comment1": "write tags for host and eventtype fields into tag::host and tag::eventtype",
      "comment2": "write new field test that contains tags for all fields",
      "comment3": "write tags for host and sourcetype into field test in the format host::<tag> or sourcetype::<tag>",
      "description": "Annotate the search results with tags. If there are fields specified only annotate tags for those fields otherwise look for tags for all fields.  If outputfield is specified, the tags for all fields will be written to this field.  Otherwise, the tags for each field will be written to a field named tag::<field>.  If outputfield is specified, inclname and inclvalue control whether or not the field name and field values are added to the output field.  By default only the tag itself is written to the outputfield.  E.g.: (<field>::)?(<value>::)?tag",
      "example1": "... | tags host eventtype",
      "example2": "... | tags outputfield=test",
      "example3": "... | tags outputfield=test inclname=t host sourcetype",
      "related": "eval",
      "shortdesc": "Annotates specified fields in your search results with tags.",
      "syntax": "tags (outputfield=<field>)? (inclname=<bool>)? (inclvalue=<bool>)? (<field> )*",
      "tags": "tags",
      "usage": "public"
   },
   "tail": {
      "category": "results::order",
      "commentcheat": "Return the last 20 results (in reverse order).",
      "description": "Returns the last n results, or 10 if no integer is specified.  The events\n             are returned in reverse order, starting at the end of the result set.",
      "examplecheat": "... | tail 20",
      "related": "head, reverse",
      "shortdesc": "Returns the last n number of specified results.",
      "syntax": "tail (<int>)?",
      "tags": "tail last bottom trailing earliest",
      "usage": "public beta"
   },
   "timechart": {
      "category": "reporting",
      "commentcheat1": "Graph the average \"thruput\" of hosts over time.",
      "commentcheat2": "Create a timechart of average \"cpu_seconds\" by \"host\", and remove data (outlying values) that may distort the timechart's axis.",
      "commentcheat3": "Calculate the average value of \"CPU\" each minute for each \"host\".",
      "commentcheat4": "Create a timechart of the count of from \"web\" sources by \"host\"",
      "commentcheat5": "Compute the product of the average \"CPU\" and average \"MEM\" each minute for each \"host\"",
      "description": "Creates a chart for a statistical aggregation applied to a field against time. When\n             the data is split by a field, each distinct value of this split-by field is a series.\n             If used with an eval-expression, the split-by-clause is required. \\p\\\n             When a where clause is not provided, you can use limit and agg options to specify\n             series filtering. If limit=0, there is no series filtering. \\p\\\n             When specifying multiple data series with a split-by-clause, you can use sep and\n             format options to construct output field names.\\p\\\n             When called without any bin-options, timechart defaults to bins=300. This finds\n             the smallest bucket size that results in no more than three hundred distinct buckets.",
      "example1": "... | timechart span=5m avg(delay) by host",
      "example2": "sourcetype=access_combined | timechart span=1m count(_raw) by product_id usenull=f",
      "example3": "sshd failed OR failure | timechart span=1m count(eventtype) by source_ip usenull=f where count>10",
      "examplecheat1": "... | timechart span=5m avg(thruput) by host",
      "examplecheat2": "... | timechart avg(cpu_seconds) by host | outlier action=tf",
      "examplecheat3": "... | timechart span=1m avg(CPU) by host",
      "examplecheat4": "... | timechart count by host",
      "examplecheat5": "... | timechart span=1m eval(avg(CPU) * avg(MEM)) by host",
      "note": "When called without any bin-options, TIMECHART assumes bins=300 has been specified. This finds the smallest bucket size that results in no more than 300 distinct buckets.",
      "related": "bucket, chart, sitimechart",
      "shortdesc": "Creates a time series chart with corresponding table of statistics.",
      "supports-multivalue": "true",
      "syntax": "timechart <timechart-command-arguments>",
      "tags": "chart graph report count dc mean avg stdev var min max mode median per_second per_minute per_hour per_day",
      "usage": "public"
   },
   "timewrap": {
      "category": "reporting",
      "comment1": "Display a timechart that has a span of 1 day for each count in a week over\nweek comparison table. Each table column, which is the series, is 1 week of time.",
      "description": "Displays, or wraps, the output of timechart so that every period of time is\n             a different series. Use the timewrap command to compare data over specific\n             time period, such as day-over-day or month-over-month. You can also compare\n             multiple time periods, periods, such as a two week period over another\n             two week period. The <timewrap-span> can be any length of time including\n             weeks and quarters. You must use the timechart command in the search before\n             you use the timewrap command.",
      "example1": "... | timechart count span=1d | timewrap 1week",
      "related": "timechart",
      "shortdesc": "Displays the output of timechart so that every period of time is a different series.",
      "syntax": "timewrap <timewrap-span> (align=(now|end))? (series=(relative|exact|short)?) (time_format=<str>)?",
      "tags": "timechart",
      "usage": "public"
   },
   "top": {
      "alias": "common",
      "category": "reporting",
      "comment1": "Return top URL values.",
      "comment2": "Return top \"user\" values for each \"host\".",
      "commentcheat": "Return the 20 most common values of the \"url\" field.",
      "description": "Finds the most frequent tuple of values of all fields in the field list, along with a count and percentage.\n           If a the optional by-clause is provided, finds the most frequent values\n           for each distinct tuple of values of the group-by fields.",
      "example1": "... | top url",
      "example2": "... | top user by host",
      "examplecheat": "... | top limit=20 url",
      "related": "rare, sitop, stats",
      "shortdesc": "Displays the most common values of a field.",
      "supports-multivalue": "true",
      "syntax": "top <top-command-arguments>",
      "tags": "top popular common many frequent typical",
      "usage": "public"
   },
   "transaction": {
      "alias": "transam",
      "category": "results::group",
      "comment1": "Collapse all events that share the same host and cookie value, that occur within 30 seconds, and do not have a\n          pause of more than 5 seconds between the events.",
      "comment2": "Group search results that share the same value of \"from\", with a maximum span of 30 seconds, and a pause between events no greater than 5 seconds into a transaction.",
      "commentcheat": "Group search results that have the same \"host\" and \"cookie\", occur within 30 seconds of each other, and do not have a pause greater than 5 seconds between each event into a transaction.",
      "description": "Groups events into transactions based on various constraints, such as the beginning\n             and ending strings or time between events. Transactions are made up of the raw text\n             (the _raw field) of each member, the time and date fields of the earliest member, as\n             well as the union of all other fields of each member.\\p\\\n             Produces two fields to the raw events, duration and eventcount. The duration value\n             is the difference between the timestamps for the first and last events in the\n             transaction. The eventcount value is the number of events in the transaction.",
      "example1": "... | transaction host,cookie maxspan=30s maxpause=5s",
      "example2": "... | transaction from maxspan=30s maxpause=5s",
      "examplecheat": "... | transaction host cookie maxspan=30s maxpause=5s",
      "related": "searchtxn",
      "shortdesc": "Groups events into transactions.",
      "supports-multivalue": "true",
      "syntax": "transaction (<field-list>)? (name=<transaction-name>)? (<txn_definition-opt>)* (<memcontrol-opt>)* (<rendering-opt>)*",
      "tags": "transaction group cluster collect gather",
      "usage": "public"
   },
   "transpose": {
      "category": "reporting",
      "comment1": "Turns the first five rows into columns",
      "comment2": "Turns the first 20 rows into columns",
      "comment3": "Turns the first five rows into columns, where the input field names are put into the output field called \"Test Name\", and the input row values for the sourcetype field will be used as the output field names.",
      "description": "Turns rows into columns (each row becomes a column).  Takes an optional integer argument that limits the number of rows we transpose (default = 5).  column_name is the name of the field in the output where the names of the fields of the inputs will go (default = \"column\").  header_field, if provided, will use the value of this field in each input row as the name of the output field for that column (default = no field provided, output fields will be named \"row 1\", \"row 2\", ...).  include_empty is an optional boolean option, that if false, will exclude any field/column in the input that had no values for any row (defaults = true).",
      "example1": "... | transpose",
      "example2": "... | transpose 20",
      "example3": "... | transpose column_name=\"Test Name\" header_field=sourcetype include_empty=false",
      "related": "fields, stats",
      "shortdesc": "Turns rows into columns.",
      "syntax": "transpose (<int>)? (column_name=<string>)? (header_field=<field>)? (include_empty=<bool>)?",
      "tags": "fields, stats",
      "usage": "public"
   },
   "trendline": {
      "category": "reporting",
      "comment1": "Computes a 5 event simple moving average for field 'foo' and write to new field 'smoothed_foo'.\n          Also computes a 10 event exponential moving average for field 'bar'. Because no AS clause is\n          specified, writes to the field 'ema10(bar)'.",
      "description": "Computes the moving averages of fields.  Current supported trend_types include\n             simple moving average (sma), exponential moving average(ema), and weighted moving average(wma)\n             The output is written to a new field where the new field name can be explicitly specified or\n             by default it is simply the trend_type + field.",
      "example1": "... | trendline sma5(foo) AS smoothed_foo ema10(bar)",
      "related": "accum, autoregress, delta, streamstats, trendline",
      "shortdesc": "Computes the moving averages of fields.",
      "syntax": "trendline (<trend_type>\"(\"<field>\")\" (as <field>)?)+",
      "tags": "average mean",
      "usage": "public"
   },
   "tstats": {
      "category": "reporting",
      "comment1": "Gets the count of all events in the mydata namespace",
      "comment2": "Returns the average of field foo in mydata where bar is specifically 'value2' and the value of baz is greater than 5.",
      "comment3": "Gives the count by source for events with host=x",
      "comment4": "Gives a timechart of all the data in your default indexes with a day granularity",
      "comment5": "Gives the median of field foo from mydata",
      "comment6": "Uses prestats mode in conjunction with append to compute the median values of foo and bar, which are in different namespaces",
      "description": "Performs statistical queries on indexed fields in tsidx files. You can select from TSIDX data in several different ways:                                  \\p\\\n             1. Normal index data: If you do not supply a FROM clause, we will select from index data in the same way as search. You are restricted to selecting\n                from your allowed indexes by role, and you can control exactly which indexes you select from in the WHERE clause. If no indexes are mentioned\n                in the WHERE clause search, we will use your default set of indexes. By default, role-based search filters are applied, but can be turned off in limits.conf. \\p\\\n             2. Data manually collected with 'tscollect': Select from your namespace with 'FROM <namespace>'. If you supplied no namespace to tscollect, the data\n                was collected into the dispatch directory of that job. In that case, you would select from that data with 'FROM sid=<tscollect-job-id>'                \\p\\\n             3. An accelerated datamodel: Select from this accelerated datamodel with 'FROM datamodel=<datamodel-name>'\n             You can provide any number of aggregates to perform, and also have the option of providing a filtering query using the WHERE keyword. This query looks\n             like a normal query you would use in the search processor. You can also provide any number of GROUPBY fields. If you are grouping by _time, you should\n             supply a timespan with 'span' for grouping the time buckets. This timespan looks like any normal timespan in Splunk, like '1hr' or '3d'. It also supports 'auto'.     \\p\\\n             Arguments:                                                                                                                                                \\i\\\n             \"prestats\": This simply outputs the answer in prestats format, in case you want to pipe the results to a                                                  \\i\\\n                         different type of processor that takes prestats output, like chart or timechart. This is very useful for                                      \\i\\\n                         creating graphs                                                                                                                               \\i\\\n             \"local\": If you set this to true it forces the processor to only be run on the search head.                                                               \\i\\\n             \"append\": Only valid in prestats mode, this allows tstats to be run to add results to an existing set of                                                  \\i\\\n                       results, instead of generating them.                                                                                                            \\i\\\n             \"summariesonly\": Only applies when selecting from an accelerated datamodel.  When false (default),                                                        \\i\\\n                              Splunk will generate results from both summarized data, as well as for data that is not                                                  \\i\\\n                              summarized. For data not summarized as TSIDX data, the full search behavior will be used                                                 \\i\\\n                              against the original index data.  If set to true, 'tstats' will only generate results from the                                           \\i\\\n                              TSIDX data that has been automatically generated by the acceleration, and nonsummarized data                                             \\i\\\n                              will not be provided.                                                                                                                    \\i\\\n             \"allow_old_summaries\": Only applies when selecting from an accelerated datamodel.  When false                                                             \\i\\\n                                    (default), Splunk only provides results from summary directories when those directories are up-to-date.                            \\i\\\n                                    In other words, if the datamodel definition has changed, we do not use those summary directories                                   \\i\\\n                                    which are older than the new definition when producing output from tstats. This default ensures                                    \\i\\\n                                    that the output from tstats will always reflect your current configuration. If this is instead                                     \\i\\\n                                    set to true, then tstats will use both current summary data as well as summary data that was                                        \\i\\\n                                    generated prior to the definition change. Essentially this is an advanced performance                                              \\i\\\n                                    feature for cases where you know that the old summaries are \"good enough\".                                                         \\i\\\n             \"chunk_size\": Advanced option. This argument controls how many events are retrieved at a time within                                                      \\i\\\n                           a single TSIDX file when answering queries. The default is 10000000. Only consider supplying a lower                                        \\i\\\n                           value for this if you find a particular query is using too much memory. The case that could cause this                                      \\i\\\n                           would be an excessively high cardinality split-by, such as grouping by several fields that have a very                                      \\i\\\n                           large amount of distinct values. Setting this value too low, however, can negatively impact the overall                                     \\i\\\n                           runtime of your query.                                                                                                                      \\p\\\n             NOTE: Except in 'append=t' mode, this is a generating processor, so it must be the first command in a search.",
      "example1": "| tstats count FROM mydata",
      "example2": "| tstats avg(foo) from mydata where bar=value2 baz>5",
      "example3": "| tstats count where host=x by source",
      "example4": "| tstats prestats=t count by _time span=1d | timechart span=1d count",
      "example5": "| tstats median(foo) from mydata",
      "example6": "| tstats prestats=t median(foo) from mydata | tstats prestats=t append=t median(bar) from otherdata | stats median(foo) median(bar)",
      "optout-in": "lite, lite_free",
      "related": "tscollect",
      "shortdesc": "Performs statistics on indexed fields in tsidx files, which could come from normal index data, tscollect data, or accelerated datamodels.",
      "syntax": "tstats (prestats=<bool>)? (local=<bool>)? (append=<bool>)? (summariesonly=<bool>)? (allow_old_summaries=<bool>)? (chunk_size=<unsigned int>)? (<stats-func>)+ (FROM <string:namespace> | sid=<string:tscollect-job-id> | datamodel=<string:datamodel-name>)? (WHERE <logical-expression>)? ((by|GROUPBY) <field-list> (span=<string:timespan>)? )?",
      "tags": "tstats tsidx projection",
      "usage": "public"
   },
   "typeahead": {
      "category": "administrative",
      "comment1": "Return typeahead information for sources in the \"_internal\" index.",
      "description": "Returns typeahead on a specified prefix. Only returns a max of \"count\" results, can be targeted to an index and restricted by time.\n             If index specifiers are provided they're used to populate the set of indexes used if no index specifiers are found in the prefix.",
      "example1": "| typeahead prefix=\"index=_internal source=\" count=10",
      "generating": "true",
      "shortdesc": "Returns typeahead on a specified prefix.",
      "syntax": "typeahead <prefix-opt> <count-opt> (<max-time-opt>)? (<index-opt>)? (<starttimeu>)? (<endtimeu>)? (<collapse-opt>)?",
      "tags": "typeahead help terms",
      "usage": "public"
   },
   "typer": {
      "category": "results::group",
      "commentcheat": "Force Splunk to apply event types that you have configured (Splunk Web automatically does this when you view the \"eventtype\" field).",
      "description": "Calculates the 'eventtype' field for search results that match a known event-type.",
      "examplecheat": "... | typer",
      "related": "typelearner",
      "shortdesc": "Calculates the eventtypes for the search results.",
      "syntax": "typer",
      "tags": "eventtype typer discover search classify",
      "usage": "public"
   },
   "union": {
      "category": "results::append",
      "comment1": "Merge events from index a and b and add different fields using eval in each case.",
      "comment2": "Append the current results with the tabular results of errors.",
      "comment3": "Search a built-in data model that is an internal server log for REST API calls and the events from index a.",
      "description": "Merges the results from two or more datasets into one dataset.",
      "example1": "| union [search index=a | eval type = \"foo\"] [search index=b | eval mytype = \"bar\"]",
      "example2": "... | chart count by category1 | union [search error | chart count by category2]",
      "example3": "| union datamodel:\"internal_server.splunkdaccess\" [search index=a]",
      "related": "multisearch, append",
      "shortdesc": "Merge multiple datasets.",
      "syntax": "union (<subsearch-options>)? <dataset> (<dataset>)*",
      "tags": "multisearch append",
      "usage": "public"
   },
   "uniq": {
      "category": "results::filter",
      "comment1": "For the current search, keep only unique results.",
      "description": "Removes any search result that is an exact duplicate with the adjacent result before it.",
      "example1": "... | uniq",
      "related": "dedup",
      "shortdesc": "Filters out repeated adjacent results.",
      "syntax": "uniq",
      "tags": "uniq unique duplicate redundant extra",
      "usage": "public"
   },
   "untable": {
      "category": "reporting",
      "comment1": "Reformat the search results.",
      "description": "Converts results from a tabular format to a format similar to stats output.  Inverse of xyseries.",
      "example1": "... | timechart avg(delay) by host | untable _time host avg_delay",
      "related": "xyseries",
      "syntax": "untable <x-field> <y-name-field> <y-data-field>",
      "tags": "convert table",
      "usage": "public"
   },
   "where": {
      "category": "results::filter",
      "comment1": "Return \"physicjobs\" events with a speed is greater than 100.",
      "comment2": "Return \"CheckPoint\" events that match the IP or is in the specified subnet.",
      "description": "Keeps only the results for which the evaluation was successful and the boolean result was true.",
      "example1": "sourcetype=physicsobjs | where distance/time > 100",
      "example2": "host=\"CheckPoint\" | where (src LIKE \"10.9.165.%\") OR cidrmatch(\"10.9.165.0/25\", dst)",
      "related": "eval search regex",
      "shortdesc": "Runs an eval expression to filter the results. The result of the expression must be Boolean.",
      "syntax": "where <eval-expression>",
      "tags": "where filter search",
      "usage": "public"
   },
   "x11": {
      "category": "reporting",
      "description": "Remove seasonal fluctuations in fields. This command has a similar purpose to the\n             trendline command, but is more sophisticated as it uses the industry popular X11 method.\n             The type option can be either 'mult' (for multiplicative) or 'add' (for additive). By default,\n             it's 'mult'. The period option should be specified if known; otherwise it is automatically computed.",
      "example1": "... | x11 foo as fubar",
      "example2": "... | x11 24(foo) as fubar",
      "example3": "... | x11 add12(foo) as fubar",
      "related": "trendline",
      "shortdesc": "Remove seasonal fluctuations in fields.",
      "syntax": "x11 <x11-func>\"(\"<fieldname>\")\" (as <newname>)?",
      "tags": "x11 deseasonal seasonal",
      "usage": "public"
   },
   "xmlkv": {
      "category": "fields::add",
      "commentcheat": "Extract field/value pairs from XML formatted data. \"xmlkv\" automatically extracts values between XML tags.",
      "description": "Finds key value pairs of the form <foo>bar</foo> where foo is the key and bar is the value from the _raw key.",
      "example1": "... | xmlkv maxinputs=10000",
      "examplecheat": "... | xmlkv",
      "related": "extract, kvform, multikv, rex, xpath",
      "shortdesc": "Extracts XML key-value pairs.",
      "syntax": "xmlkv <maxinputs-opt>",
      "tags": "extract xml",
      "usage": "public"
   },
   "xmlunescape": {
      "category": "formatting",
      "commentcheat": "Un-escape all XML characters.",
      "description": "Un-escapes XML entity references (for: &, <, and >) back to their corresponding characters (e.g., \"&amp;\" -> \"&\").",
      "examplecheat": "... | xmlunescape",
      "shortdesc": "Un-escapes XML characters.",
      "syntax": "xmlunescape <maxinputs-opt>",
      "tags": "unescape xml escape",
      "usage": "public"
   },
   "xpath": {
      "category": "fields::add",
      "comment1": "pull out the name of a book from xml, using the relative path of //book",
      "comment2": "pull out the name of a book from xml, using the full path of /data/book",
      "description": "Sets the value of OUTFIELD to the value of the XPATH applied to FIELD.  If no value could be set, the DEFAULT value is set.  FIELD defaults to \"_raw\"; OUTFIELD, to \"xpath\"; and DEFAULT, to not setting a default value.  The field value is wrapped in a \"<data>...</data>\" tags so that the field value is a valid xml, even if it contains some none xml.",
      "example1": "sourcetype=\"books.xml\" | xpath \"//book/@name\"  outfield=name",
      "example2": "sourcetype=\"books.xml\" | xpath \"/data/book/@name\"  outfield=name",
      "related": "extract, kvform, multikv, rex, xmlkv",
      "shortdesc": "Extracts the xpath value from FIELD and sets the OUTFIELD attribute.",
      "syntax": "xpath <string:xpath> (field=<field>)? (outfield=<field>)? (default=<string>)?",
      "tags": "xml extract",
      "usage": "public"
   },
   "xyseries": {
      "alias": "maketable",
      "category": "reporting",
      "comment1": "Reformat the search results.",
      "description": "Converts results into a format suitable for graphing.  If multiple\n             y-data-fields are specified, each column name is the\n             the y-data-field name followed by the sep string (default is \": \")\n             and then the value of the y-name-field it applies to.\n             If the grouped option is set to true (false by default),\n             then the input is assumed to be sorted by the value of the\n             <x-field> and multi-file input is allowed.",
      "example1": "... | xyseries delay host_type host",
      "related": "untable",
      "shortdesc": "Converts results into a format suitable for graphing.",
      "syntax": "xyseries (grouped=<bool>)? <x-field> <y-name-field> (<y-data-field>)+ (sep=<string>)? (format=<string>)?",
      "tags": "convert graph",
      "usage": "public"
   }
}

